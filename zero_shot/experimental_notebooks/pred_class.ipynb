{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.491, 0.482, 0.446],\n",
    "            std= [0.247, 0.243, 0.261]\n",
    "        )]) # TODO: Automate calculation given a dataset\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./raid/data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./raid/data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(in_features=512, out_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 1 ##########\n",
      "Loss: 2.4020590311288834 | Acc: 13.875 | 111/800\n",
      "Loss: 2.260924884676933 | Acc: 16.1875 | 259/1600\n",
      "Loss: 2.1441143172979356 | Acc: 18.333333333333332 | 440/2400\n",
      "Loss: 2.122776813209057 | Acc: 20.0 | 640/3200\n",
      "Loss: 2.0903451085090636 | Acc: 20.525 | 821/4000\n",
      "Loss: 2.047880454957485 | Acc: 21.541666666666668 | 1034/4800\n",
      "Loss: 2.0258519053459167 | Acc: 22.375 | 1253/5600\n",
      "Loss: 2.014839940965176 | Acc: 22.71875 | 1454/6400\n",
      "Loss: 1.985060502886772 | Acc: 23.34722222222222 | 1681/7200\n",
      "Loss: 1.9067488738894462 | Acc: 24.1125 | 1929/8000\n",
      "Loss: 1.8706936863064767 | Acc: 24.852272727272727 | 2187/8800\n",
      "Loss: 1.8785923615098 | Acc: 25.395833333333332 | 2438/9600\n",
      "Loss: 1.877520509660244 | Acc: 26.0 | 2704/10400\n",
      "Loss: 1.8314386662840842 | Acc: 26.642857142857142 | 2984/11200\n",
      "Loss: 1.811381601393223 | Acc: 27.283333333333335 | 3274/12000\n",
      "Loss: 1.8709281343221664 | Acc: 27.59375 | 3532/12800\n",
      "Loss: 1.8225640466809272 | Acc: 28.00735294117647 | 3809/13600\n",
      "Loss: 1.7717475435137748 | Acc: 28.4375 | 4095/14400\n",
      "Loss: 1.7240994760394097 | Acc: 28.967105263157894 | 4403/15200\n",
      "Loss: 1.7256451608240604 | Acc: 29.39375 | 4703/16000\n",
      "Loss: 1.7825616642832756 | Acc: 29.654761904761905 | 4982/16800\n",
      "Loss: 1.7264711648225783 | Acc: 30.073863636363637 | 5293/17600\n",
      "Loss: 1.7682229933142661 | Acc: 30.33695652173913 | 5582/18400\n",
      "Loss: 1.5980885329842567 | Acc: 30.890625 | 5931/19200\n",
      "Loss: 1.7200657790899276 | Acc: 31.225 | 6245/20000\n",
      "Loss: 1.6478630873560904 | Acc: 31.64423076923077 | 6582/20800\n",
      "Loss: 1.7333968383073808 | Acc: 31.828703703703702 | 6875/21600\n",
      "Loss: 1.6496257972717285 | Acc: 32.160714285714285 | 7204/22400\n",
      "Loss: 1.5776865509152413 | Acc: 32.547413793103445 | 7551/23200\n",
      "Loss: 1.528272297680378 | Acc: 33.0 | 7920/24000\n",
      "Loss: 1.6021889212727547 | Acc: 33.34274193548387 | 8269/24800\n",
      "Loss: 1.5371155381202697 | Acc: 33.6640625 | 8618/25600\n",
      "Loss: 1.5914366787672043 | Acc: 33.928030303030305 | 8957/26400\n",
      "Loss: 1.4791259175539018 | Acc: 34.29779411764706 | 9329/27200\n",
      "Loss: 1.5008977034687996 | Acc: 34.628571428571426 | 9696/28000\n",
      "Loss: 1.4845350256562233 | Acc: 35.0 | 10080/28800\n",
      "Loss: 1.504544303715229 | Acc: 35.30067567567568 | 10449/29600\n",
      "Loss: 1.4881526041030884 | Acc: 35.64473684210526 | 10836/30400\n",
      "Loss: 1.4658898279070853 | Acc: 35.98397435897436 | 11227/31200\n",
      "Loss: 1.4274965399503707 | Acc: 36.26875 | 11606/32000\n",
      "Loss: 1.333133516460657 | Acc: 36.66158536585366 | 12025/32800\n",
      "Loss: 1.5780086144804955 | Acc: 36.8125 | 12369/33600\n",
      "Loss: 1.455263386964798 | Acc: 37.093023255813954 | 12760/34400\n",
      "Loss: 1.3634934717416762 | Acc: 37.38920454545455 | 13161/35200\n",
      "Loss: 1.4399152520298957 | Acc: 37.66388888888889 | 13559/36000\n",
      "Loss: 1.3648172283172608 | Acc: 37.97282608695652 | 13974/36800\n",
      "Loss: 1.3607686208188534 | Acc: 38.284574468085104 | 14395/37600\n",
      "Loss: 1.3421308685839177 | Acc: 38.544270833333336 | 14801/38400\n",
      "Loss: 1.3168189042806626 | Acc: 38.85204081632653 | 15230/39200\n",
      "Loss: 1.4186092448234557 | Acc: 39.055 | 15622/40000\n",
      "Loss: 1.366338876634836 | Acc: 39.34068627450981 | 16051/40800\n",
      "Loss: 1.2761193424463273 | Acc: 39.62980769230769 | 16486/41600\n",
      "Loss: 1.3483486744761466 | Acc: 39.886792452830186 | 16912/42400\n",
      "Loss: 1.2665428081154824 | Acc: 40.175925925925924 | 17356/43200\n",
      "Loss: 1.3088166251778603 | Acc: 40.40909090909091 | 17780/44000\n",
      "Loss: 1.35833426207304 | Acc: 40.642857142857146 | 18208/44800\n",
      "Loss: 1.2234334960579871 | Acc: 40.88815789473684 | 18645/45600\n",
      "Loss: 1.2891444003582 | Acc: 41.116379310344826 | 19078/46400\n",
      "Loss: 1.2254782877862453 | Acc: 41.36440677966102 | 19524/47200\n",
      "Loss: 1.2871181659400464 | Acc: 41.60208333333333 | 19969/48000\n",
      "Loss: 1.3128210936486722 | Acc: 41.77049180327869 | 20384/48800\n",
      "Loss: 1.2150230126082897 | Acc: 42.002016129032256 | 20833/49600\n",
      "########## 2 ##########\n",
      "Loss: 1.1808279693126678 | Acc: 58.875 | 471/800\n",
      "Loss: 1.1422484220564366 | Acc: 59.8125 | 957/1600\n",
      "Loss: 1.1998156768083572 | Acc: 58.666666666666664 | 1408/2400\n",
      "Loss: 1.1832532340288162 | Acc: 58.53125 | 1873/3200\n",
      "Loss: 1.2476327043771744 | Acc: 57.975 | 2319/4000\n",
      "Loss: 1.051070067435503 | Acc: 58.666666666666664 | 2816/4800\n",
      "Loss: 1.1958299204707146 | Acc: 58.410714285714285 | 3271/5600\n",
      "Loss: 1.1887006707489491 | Acc: 58.375 | 3736/6400\n",
      "Loss: 1.1691202332079411 | Acc: 58.583333333333336 | 4218/7200\n",
      "Loss: 1.180430716276169 | Acc: 58.7 | 4696/8000\n",
      "Loss: 1.099878286421299 | Acc: 58.89772727272727 | 5183/8800\n",
      "Loss: 1.1423077514767648 | Acc: 58.947916666666664 | 5659/9600\n",
      "Loss: 1.190859882235527 | Acc: 58.97115384615385 | 6133/10400\n",
      "Loss: 1.167293013483286 | Acc: 59.044642857142854 | 6613/11200\n",
      "Loss: 1.1011237458884715 | Acc: 59.15833333333333 | 7099/12000\n",
      "Loss: 1.1237184242904186 | Acc: 59.2421875 | 7583/12800\n",
      "Loss: 1.0975017102062703 | Acc: 59.463235294117645 | 8087/13600\n",
      "Loss: 1.0853205500543117 | Acc: 59.50694444444444 | 8569/14400\n",
      "Loss: 1.0370726633071898 | Acc: 59.75 | 9082/15200\n",
      "Loss: 1.044405121654272 | Acc: 59.95 | 9592/16000\n",
      "Loss: 1.0793371285498141 | Acc: 60.029761904761905 | 10085/16800\n",
      "Loss: 1.0462699195742606 | Acc: 60.125 | 10582/17600\n",
      "Loss: 1.215952051728964 | Acc: 60.07608695652174 | 11054/18400\n",
      "Loss: 1.0336989206075669 | Acc: 60.197916666666664 | 11558/19200\n",
      "Loss: 1.045951896160841 | Acc: 60.32 | 12064/20000\n",
      "Loss: 1.0461722669005393 | Acc: 60.44230769230769 | 12572/20800\n",
      "Loss: 1.069900574684143 | Acc: 60.46296296296296 | 13060/21600\n",
      "Loss: 0.9815329426527023 | Acc: 60.660714285714285 | 13588/22400\n",
      "Loss: 1.084407128840685 | Acc: 60.75 | 14094/23200\n",
      "Loss: 1.03661598905921 | Acc: 60.82083333333333 | 14597/24000\n",
      "Loss: 0.9625179886817932 | Acc: 61.020161290322584 | 15133/24800\n",
      "Loss: 1.0285284668207169 | Acc: 61.12890625 | 15649/25600\n",
      "Loss: 1.119874410033226 | Acc: 61.128787878787875 | 16138/26400\n",
      "Loss: 1.0050082702934742 | Acc: 61.24264705882353 | 16658/27200\n",
      "Loss: 1.0037127144634723 | Acc: 61.35 | 17178/28000\n",
      "Loss: 0.9570111261308193 | Acc: 61.482638888888886 | 17707/28800\n",
      "Loss: 1.0494401295483113 | Acc: 61.55067567567568 | 18219/29600\n",
      "Loss: 1.0253972923755645 | Acc: 61.64802631578947 | 18741/30400\n",
      "Loss: 1.0042979489266872 | Acc: 61.717948717948715 | 19256/31200\n",
      "Loss: 1.1019817987084388 | Acc: 61.675 | 19736/32000\n",
      "Loss: 0.9600030919909477 | Acc: 61.75914634146341 | 20257/32800\n",
      "Loss: 0.9825706000626088 | Acc: 61.851190476190474 | 20782/33600\n",
      "Loss: 0.980724084675312 | Acc: 61.93313953488372 | 21305/34400\n",
      "Loss: 0.9752801987528801 | Acc: 62.03409090909091 | 21836/35200\n",
      "Loss: 1.016729515939951 | Acc: 62.075 | 22347/36000\n",
      "Loss: 0.9753047426044941 | Acc: 62.14673913043478 | 22870/36800\n",
      "Loss: 0.9852459365129471 | Acc: 62.23936170212766 | 23402/37600\n",
      "Loss: 0.9432176224887371 | Acc: 62.34375 | 23940/38400\n",
      "Loss: 0.9346031691133976 | Acc: 62.43112244897959 | 24473/39200\n",
      "Loss: 0.9744362938404083 | Acc: 62.4975 | 24999/40000\n",
      "Loss: 0.9593528407812119 | Acc: 62.583333333333336 | 25534/40800\n",
      "Loss: 0.9159856835007667 | Acc: 62.71634615384615 | 26090/41600\n",
      "Loss: 0.926651656627655 | Acc: 62.83018867924528 | 26640/42400\n",
      "Loss: 0.947284637093544 | Acc: 62.91898148148148 | 27181/43200\n",
      "Loss: 0.9737081030011177 | Acc: 62.972727272727276 | 27708/44000\n",
      "Loss: 0.9676392285525799 | Acc: 63.049107142857146 | 28246/44800\n",
      "Loss: 0.9787163710594178 | Acc: 63.098684210526315 | 28773/45600\n",
      "Loss: 0.9122217020392418 | Acc: 63.213362068965516 | 29331/46400\n",
      "Loss: 0.9341893428564072 | Acc: 63.269067796610166 | 29863/47200\n",
      "Loss: 0.9714049470424652 | Acc: 63.32083333333333 | 30394/48000\n",
      "Loss: 0.9095931905508041 | Acc: 63.38319672131148 | 30931/48800\n",
      "Loss: 0.9002144904434681 | Acc: 63.47782258064516 | 31485/49600\n",
      "########## 3 ##########\n",
      "Loss: 0.8762271882593632 | Acc: 70.125 | 561/800\n",
      "Loss: 0.8785135646164417 | Acc: 69.8125 | 1117/1600\n",
      "Loss: 0.8535880164802074 | Acc: 69.875 | 1677/2400\n",
      "Loss: 0.8184907045960427 | Acc: 70.5625 | 2258/3200\n",
      "Loss: 0.781199276894331 | Acc: 71.05 | 2842/4000\n",
      "Loss: 0.8869962306320667 | Acc: 70.6875 | 3393/4800\n",
      "Loss: 0.8065877403318882 | Acc: 70.75 | 3962/5600\n",
      "Loss: 0.8364338567852974 | Acc: 70.765625 | 4529/6400\n",
      "Loss: 0.9135961931943893 | Acc: 70.58333333333333 | 5082/7200\n",
      "Loss: 0.852116858959198 | Acc: 70.6375 | 5651/8000\n",
      "Loss: 0.8267278751730919 | Acc: 70.68181818181819 | 6220/8800\n",
      "Loss: 0.8774915082752704 | Acc: 70.54166666666667 | 6772/9600\n",
      "Loss: 0.7925819367170334 | Acc: 70.6923076923077 | 7352/10400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7927097563445568 | Acc: 70.78571428571429 | 7928/11200\n",
      "Loss: 0.9105615282058716 | Acc: 70.81666666666666 | 8498/12000\n",
      "Loss: 0.8148881486058235 | Acc: 70.90625 | 9076/12800\n",
      "Loss: 0.9042763666808605 | Acc: 70.69852941176471 | 9615/13600\n",
      "Loss: 0.8561027243733406 | Acc: 70.67361111111111 | 10177/14400\n",
      "Loss: 0.823942962884903 | Acc: 70.63815789473684 | 10737/15200\n",
      "Loss: 0.8020720602571965 | Acc: 70.725 | 11316/16000\n",
      "Loss: 0.9100853070616722 | Acc: 70.64285714285714 | 11868/16800\n",
      "Loss: 0.83401825979352 | Acc: 70.58522727272727 | 12423/17600\n",
      "Loss: 0.8183437976241111 | Acc: 70.68478260869566 | 13006/18400\n",
      "Loss: 0.844349091053009 | Acc: 70.72395833333333 | 13579/19200\n",
      "Loss: 0.7919922173023224 | Acc: 70.825 | 14165/20000\n",
      "Loss: 0.8757037968933582 | Acc: 70.77403846153847 | 14721/20800\n",
      "Loss: 0.8188944709300995 | Acc: 70.81018518518519 | 15295/21600\n",
      "Loss: 0.8035459810495377 | Acc: 70.89285714285714 | 15880/22400\n",
      "Loss: 0.8067472165822983 | Acc: 70.95689655172414 | 16462/23200\n",
      "Loss: 0.7730268770456314 | Acc: 71.00833333333334 | 17042/24000\n",
      "Loss: 0.7902854898571968 | Acc: 71.07661290322581 | 17627/24800\n",
      "Loss: 0.7200032407045365 | Acc: 71.203125 | 18228/25600\n",
      "Loss: 0.7843170005083084 | Acc: 71.26893939393939 | 18815/26400\n",
      "Loss: 0.8591211293637753 | Acc: 71.23529411764706 | 19376/27200\n",
      "Loss: 0.7969312979280949 | Acc: 71.29285714285714 | 19962/28000\n",
      "Loss: 0.7566887004673482 | Acc: 71.35763888888889 | 20551/28800\n",
      "Loss: 0.7942494405806064 | Acc: 71.38175675675676 | 21129/29600\n",
      "Loss: 0.8322703164815902 | Acc: 71.40789473684211 | 21708/30400\n",
      "Loss: 0.7665195621550083 | Acc: 71.49038461538461 | 22305/31200\n",
      "Loss: 0.8208369500935078 | Acc: 71.475 | 22872/32000\n",
      "Loss: 0.7786126083135605 | Acc: 71.5060975609756 | 23454/32800\n",
      "Loss: 0.72178710475564 | Acc: 71.62202380952381 | 24065/33600\n",
      "Loss: 0.7752475099265576 | Acc: 71.6686046511628 | 24654/34400\n",
      "Loss: 0.7720856356620789 | Acc: 71.6903409090909 | 25235/35200\n",
      "Loss: 0.7134265646338462 | Acc: 71.79444444444445 | 25846/36000\n",
      "Loss: 0.7575511866807938 | Acc: 71.8695652173913 | 26448/36800\n",
      "Loss: 0.7415400955080986 | Acc: 71.96808510638297 | 27060/37600\n",
      "Loss: 0.7067625844478607 | Acc: 72.02864583333333 | 27659/38400\n",
      "Loss: 0.7350474759936333 | Acc: 72.07908163265306 | 28255/39200\n",
      "Loss: 0.7535362502932549 | Acc: 72.12 | 28848/40000\n",
      "Loss: 0.7588145534694195 | Acc: 72.12254901960785 | 29426/40800\n",
      "Loss: 0.7168125040829182 | Acc: 72.19711538461539 | 30034/41600\n",
      "Loss: 0.7376258750259876 | Acc: 72.21462264150944 | 30619/42400\n",
      "Loss: 0.7313237509131432 | Acc: 72.26388888888889 | 31218/43200\n",
      "Loss: 0.781575126349926 | Acc: 72.26818181818182 | 31798/44000\n",
      "Loss: 0.725939903408289 | Acc: 72.29241071428571 | 32387/44800\n",
      "Loss: 0.7712445873022079 | Acc: 72.31359649122807 | 32975/45600\n",
      "Loss: 0.7529295152425766 | Acc: 72.34913793103448 | 33570/46400\n",
      "Loss: 0.7084549477696419 | Acc: 72.42796610169492 | 34186/47200\n",
      "Loss: 0.722765938937664 | Acc: 72.47708333333334 | 34789/48000\n",
      "Loss: 0.8403941896557808 | Acc: 72.43852459016394 | 35350/48800\n",
      "Loss: 0.742155222594738 | Acc: 72.48790322580645 | 35954/49600\n",
      "########## 4 ##########\n",
      "Loss: 0.6881083264946938 | Acc: 76.375 | 611/800\n",
      "Loss: 0.6330850966274738 | Acc: 78.0 | 1248/1600\n",
      "Loss: 0.6815317088365554 | Acc: 76.95833333333333 | 1847/2400\n",
      "Loss: 0.682855776399374 | Acc: 77.28125 | 2473/3200\n",
      "Loss: 0.6552925279736519 | Acc: 77.35 | 3094/4000\n",
      "Loss: 0.6636554849147797 | Acc: 77.20833333333333 | 3706/4800\n",
      "Loss: 0.6099259319901467 | Acc: 77.30357142857143 | 4329/5600\n",
      "Loss: 0.6415678465366363 | Acc: 77.21875 | 4942/6400\n",
      "Loss: 0.6789128562808037 | Acc: 77.16666666666667 | 5556/7200\n",
      "Loss: 0.6307305717468261 | Acc: 77.35 | 6188/8000\n",
      "Loss: 0.6307464097440243 | Acc: 77.30681818181819 | 6803/8800\n",
      "Loss: 0.623760911077261 | Acc: 77.375 | 7428/9600\n",
      "Loss: 0.6993483462929726 | Acc: 77.25 | 8034/10400\n",
      "Loss: 0.6443105378746986 | Acc: 77.30357142857143 | 8658/11200\n",
      "Loss: 0.6691915428638459 | Acc: 77.275 | 9273/12000\n",
      "Loss: 0.6477161824703217 | Acc: 77.3203125 | 9897/12800\n",
      "Loss: 0.5947659707069397 | Acc: 77.45588235294117 | 10534/13600\n",
      "Loss: 0.6910734708607197 | Acc: 77.45138888888889 | 11153/14400\n",
      "Loss: 0.6874408248066902 | Acc: 77.40131578947368 | 11765/15200\n",
      "Loss: 0.6712930050492286 | Acc: 77.43125 | 12389/16000\n",
      "Loss: 0.6833720014989376 | Acc: 77.3452380952381 | 12994/16800\n",
      "Loss: 0.6672578039765358 | Acc: 77.42045454545455 | 13626/17600\n",
      "Loss: 0.6033814187347889 | Acc: 77.4836956521739 | 14257/18400\n",
      "Loss: 0.6462962852418422 | Acc: 77.49479166666667 | 14879/19200\n",
      "Loss: 0.6876684768497944 | Acc: 77.525 | 15505/20000\n",
      "Loss: 0.6865627743303776 | Acc: 77.46634615384616 | 16113/20800\n",
      "Loss: 0.7121578776836395 | Acc: 77.36574074074075 | 16711/21600\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(15):  # loop over the dataset multiple times\n",
    "    print(\"########## {} ##########\".format(epoch+1))\n",
    "    train_loss = 0.0\n",
    "    total  = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print(\"Loss: {} | Acc: {} | {}/{}\".format(train_loss/200, 100.*correct/total, correct, total))\n",
    "            train_loss = 0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./raid/trained_models/pred_class.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}