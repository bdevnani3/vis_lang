{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "homeless-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import models\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "class Base:\n",
    "    \"\"\"\n",
    "    Base class for handling experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path: str = \".\",\n",
    "        project_path: str = \"vis_lang\",\n",
    "        variant_name: str = \"base\",\n",
    "        epochs: int = 200,\n",
    "    ):\n",
    "        self.root_path = root_path\n",
    "        self.project_path = project_path\n",
    "        self.variant_name = variant_name\n",
    "        self.epochs = epochs\n",
    "        self.checkpoints_path = os.path.join(\n",
    "            self.root_path, self.project_path, self.variant_name, \"models/\"\n",
    "        )\n",
    "        make_dirs(self.checkpoints_path)\n",
    "        self.plots_path = os.path.join(\n",
    "            self.root_path, self.project_path, self.variant_name, \"plots/\"\n",
    "        )\n",
    "        make_dirs(self.plots_path)\n",
    "\n",
    "        self.class_names = None\n",
    "        self.train_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "        self.model = None\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.train_accuracy = []\n",
    "        self.test_losses = []\n",
    "        self.test_accuracy = []\n",
    "        self.best_accuracy = 0\n",
    "        self.min_loss = np.inf\n",
    "\n",
    "        # Logs\n",
    "        self.logs_path = os.path.join(\n",
    "            self.root_path, project_path, self.variant_name, \"logs.txt\"\n",
    "        )\n",
    "\n",
    "    def init_datasets(self):\n",
    "        \"\"\"\n",
    "        Populate self.train_dataset and self.test_dataset\n",
    "        \"\"\"\n",
    "        NotImplementedError\n",
    "\n",
    "    def init_dataloaders(self):\n",
    "        \"\"\"\n",
    "        Populate self.train_loader and self.test_loader with desired datasets.\n",
    "        \"\"\"\n",
    "        NotImplementedError\n",
    "\n",
    "    def set_up_model_architecture(self, num_features_in_last_layer: int):\n",
    "        \"\"\"\n",
    "        Set up architecture of the model. Since we will most likely be altering the final\n",
    "        layer of pre-existing architectures, this supports that functionality. Initialize\n",
    "        self.model.\n",
    "        \"\"\"\n",
    "        model = models.resnet18()\n",
    "        model.linear = nn.Linear(\n",
    "            in_features=512, out_features=num_features_in_last_layer\n",
    "        )\n",
    "        self.model = model\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "\n",
    "    def init_model_helpers(self, criterion):\n",
    "        self.criterion = criterion()\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            self.optimizer, milestones=[150, 250, 350], gamma=0.1\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.criterion.cuda()\n",
    "\n",
    "    def train_single_epoch(self, epoch_idx):\n",
    "        \"\"\"\n",
    "        Ensure to update self.train_losses & self.train_accuracy\n",
    "        :param epoch_idx: Index of the epoch\n",
    "        :param train_loader: dataloader object for the training dataset\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        train_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.model.train()\n",
    "        for i, data in enumerate(self.train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            device = get_device()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.calc_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += self.num_correct_preds(outputs, labels)\n",
    "\n",
    "        epoch_loss = train_loss / len(self.train_loader)\n",
    "        epoch_accuracy = correct * 100 / total\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(\n",
    "            f\"Training: Epoch {epoch_idx} || Loss: {epoch_loss:7.3f} || Accuracy: {epoch_accuracy:6.2f}% || Time: {elapsed:6.2f}\"\n",
    "        )\n",
    "\n",
    "        self.train_losses.append(epoch_loss)\n",
    "        self.train_accuracy.append(epoch_accuracy)\n",
    "\n",
    "    def num_correct_preds(self, outputs, labels):\n",
    "        _, predicted = outputs.max(1)\n",
    "        return predicted.eq(labels).sum().item()\n",
    "\n",
    "    def calc_loss(self, outputs, labels):\n",
    "        return self.criterion(outputs, labels)\n",
    "\n",
    "    def validate_single_epoch(self, epoch_idx):\n",
    "        \"\"\"\n",
    "        Ensure to update self.test_losses & self.test_accuracy\n",
    "        :param epoch_idx: Index of the epoch\n",
    "        :param test_loader: dataloader object for the test dataset\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        test_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.test_loader, 0):\n",
    "                inputs, labels = data\n",
    "                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.calc_loss(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += self.num_correct_preds(outputs, labels)\n",
    "\n",
    "        epoch_loss = test_loss / len(self.test_loader)\n",
    "        epoch_accuracy = correct * 100 / total\n",
    "\n",
    "        state = {\n",
    "            \"net\": self.model.state_dict(),\n",
    "            \"acc\": epoch_accuracy,\n",
    "            \"epoch\": epoch_idx,\n",
    "            \"loss\": epoch_loss,\n",
    "        }\n",
    "        if self.best_accuracy < epoch_accuracy:\n",
    "            self.best_accuracy = epoch_accuracy\n",
    "            print(\n",
    "                f\"Saving model with acc: {epoch_accuracy:7.3f}, loss: {epoch_loss:6.2f}, epoch: {epoch_idx}\"\n",
    "            )\n",
    "            torch.save(\n",
    "                state,\n",
    "                os.path.join(self.checkpoints_path, \"cifar10_base_best_acc.pth\"),\n",
    "            )\n",
    "\n",
    "        if self.min_loss > epoch_loss:\n",
    "            self.min_loss = epoch_loss\n",
    "            print(\n",
    "                f\"Saving model with acc: {epoch_accuracy:7.3f}, loss: {epoch_loss:6.2f}, epoch: {epoch_idx}\"\n",
    "            )\n",
    "            torch.save(\n",
    "                state,\n",
    "                os.path.join(self.checkpoints_path, \"cifar10_base_best_loss.pth\"),\n",
    "            )\n",
    "        self.test_losses.append(epoch_loss)\n",
    "        self.test_accuracy.append(epoch_accuracy)\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        print(\"Started Training\")\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.train_single_epoch(epoch)\n",
    "            self.validate_single_epoch(epoch)\n",
    "            self.scheduler.step()\n",
    "\n",
    "        print(\"Finished Training\")\n",
    "\n",
    "        print(\"Training Loss: \", self.train_losses)\n",
    "        print(\"Training Accuracy: \", self.train_accuracy)\n",
    "        print(\"Test Loss: \", self.test_losses)\n",
    "        print(\"Test Accuracy: \", self.test_accuracy)\n",
    "\n",
    "        self.export_plots()\n",
    "        self.export_data()\n",
    "\n",
    "    def export_data(self):\n",
    "\n",
    "        filename = os.path.join(\n",
    "            self.root_path, self.project_path, self.variant_name, \"raw_data\"\n",
    "        )\n",
    "\n",
    "        print(f\"Saving data at {filename}\")\n",
    "\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"Train Loss: \" + str(self.train_losses) + \"\\n\")\n",
    "            f.write(\"Train Acc: \" + str(self.train_accuracy) + \"\\n\")\n",
    "            f.write(\"Test Loss: \" + str(self.test_losses) + \"\\n\")\n",
    "            f.write(\"Test Acc: \" + str(self.test_accuracy) + \"\\n\")\n",
    "\n",
    "    def export_plots(self):\n",
    "\n",
    "        print(f\"Saving plots at {self.plots_path}\")\n",
    "\n",
    "        train_losses_fig = plt.figure()\n",
    "        plt.plot(self.train_losses)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Train Loss\")\n",
    "        train_losses_fig.savefig(os.path.join(self.plots_path, \"train_loss.png\"))\n",
    "\n",
    "        test_losses_fig = plt.figure()\n",
    "        plt.plot(self.test_losses)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Test Loss\")\n",
    "        test_losses_fig.savefig(os.path.join(self.plots_path, \"test_loss.png\"))\n",
    "\n",
    "        train_acc_fig = plt.figure()\n",
    "        plt.plot(self.train_accuracy)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Train Acc\")\n",
    "        train_acc_fig.savefig(os.path.join(self.plots_path, \"train_acc.png\"))\n",
    "\n",
    "        test_acc_fig = plt.figure()\n",
    "        plt.plot(self.test_accuracy)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Test Acc\")\n",
    "        test_acc_fig.savefig(os.path.join(self.plots_path, \"test_acc.png\"))\n",
    "\n",
    "\n",
    "# Utils\n",
    "\n",
    "\n",
    "def make_dirs(path: str):\n",
    "    \"\"\" Why is this not how the standard library works? \"\"\"\n",
    "    path = os.path.split(path)[0]\n",
    "    if path != \"\":\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "subject-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "other-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "class Cifar10(Base):\n",
    "    def __init__(self, root_path, variant_name=\"cifar10_base\", epochs=200):\n",
    "        super(Cifar10, self).__init__(\n",
    "            root_path=root_path, variant_name=variant_name, epochs=epochs\n",
    "        )\n",
    "\n",
    "    def init_datasets(self):\n",
    "\n",
    "        transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "            ]\n",
    "        )\n",
    "        transform_test = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        dataset_args = {\"root\": os.path.join(self.root_path, \"data\"), \"download\": True}\n",
    "        self.train_dataset = datasets.CIFAR10(\n",
    "            **dataset_args,\n",
    "            train=True,\n",
    "            transform=transform_train,\n",
    "        )\n",
    "\n",
    "        self.class_names = self.train_dataset.classes\n",
    "\n",
    "        self.test_dataset = datasets.CIFAR10(\n",
    "            **dataset_args,\n",
    "            train=False,\n",
    "            transform=transform_test,\n",
    "        )\n",
    "\n",
    "    def init_dataloaders(self):\n",
    "        dataloader_args = {\"batch_size\": 64, \"shuffle\": True, \"num_workers\": 2}\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset, **dataloader_args\n",
    "        )\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.test_dataset, **dataloader_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broke-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "class Cifar10Emb(Cifar10):\n",
    "    def __init__(self, root_path, variant_name=\"cifar10_emb\", epochs=200):\n",
    "        super(Cifar10Emb, self).__init__(\n",
    "            root_path=root_path, variant_name=variant_name, epochs=epochs\n",
    "        )\n",
    "\n",
    "    def find_closest_words(\n",
    "        self, word_lookup: torch.Tensor, x: torch.Tensor, mode: str = \"l2\"\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given a size [N, c] lookup table (N classes, c channels per vector) and a set of [M, c] vectors to look up,\n",
    "        returns a size [M] vector of indices from 0 to N-1 containing the closest vector in the lookup for that input.\n",
    "\n",
    "        Modes:\n",
    "            l2     - Computes pairwise L2 distance and chooses the lowest one.\n",
    "            cossim - Computs pairwise cosine similarity, and chooses the most similar. (Not implemented)\n",
    "        \"\"\"\n",
    "        N, c = word_lookup.shape\n",
    "        M, c2 = x.shape\n",
    "\n",
    "        assert (\n",
    "            c == c2\n",
    "        ), \"The lookup should have the same number of channels as the input.\"\n",
    "\n",
    "        if mode == \"l2\":\n",
    "            return (\n",
    "                ((word_lookup[None, :, :] - x[:, None, :]) ** 2)\n",
    "                .sum(dim=-1)\n",
    "                .argmin(dim=-1)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def init_word_lookup(self):\n",
    "        # We only need to lazily initialize this once. Don't reinitialize it if it's already been initialized.\n",
    "        word_vectors = gensim.downloader.load(name=\"word2vec-google-news-300\")\n",
    "\n",
    "        # Note: we store the word lookup in the model, not the datset because\n",
    "        #   1.) The word lookup should be on the same device as the model\n",
    "        #   2.) If using multiple GPUs, the model will get duplicated to each device, but the dataset won't\n",
    "        #   3.) The word model (i.e., textual feature encoder) is a property of the model not the dataset\n",
    "        self.model.word_lookup = torch.from_numpy(\n",
    "            np.stack([word_vectors[_class] for _class in self.class_names])\n",
    "        ).to(get_device())\n",
    "\n",
    "    def num_correct_preds(self, outputs, labels):\n",
    "        return (\n",
    "            (self.find_closest_words(self.model.word_lookup, outputs) == labels)\n",
    "            .sum()\n",
    "            .item()\n",
    "        )\n",
    "\n",
    "    def calc_loss(self, outputs, labels):\n",
    "        return self.criterion(outputs, self.model.word_lookup[labels])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interstate-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Here\n",
      "Started Training\n",
      "Training: Epoch 0 || Loss:   0.027 || Accuracy:  26.30% || Time:  23.63\n",
      "Saving model with acc:  31.825, loss:   0.02, epoch: 0\n",
      "Saving model with acc:  31.825, loss:   0.02, epoch: 0\n",
      "Training: Epoch 1 || Loss:   0.018 || Accuracy:  33.72% || Time:  23.47\n",
      "Saving model with acc:  36.617, loss:   0.02, epoch: 1\n",
      "Saving model with acc:  36.617, loss:   0.02, epoch: 1\n",
      "Training: Epoch 2 || Loss:   0.016 || Accuracy:  38.52% || Time:  23.38\n",
      "Saving model with acc:  41.055, loss:   0.02, epoch: 2\n",
      "Saving model with acc:  41.055, loss:   0.02, epoch: 2\n",
      "Training: Epoch 3 || Loss:   0.015 || Accuracy:  43.31% || Time:  23.46\n",
      "Saving model with acc:  47.295, loss:   0.01, epoch: 3\n",
      "Saving model with acc:  47.295, loss:   0.01, epoch: 3\n",
      "Training: Epoch 4 || Loss:   0.014 || Accuracy:  49.04% || Time:  23.51\n",
      "Saving model with acc:  52.608, loss:   0.01, epoch: 4\n",
      "Saving model with acc:  52.608, loss:   0.01, epoch: 4\n",
      "Training: Epoch 5 || Loss:   0.013 || Accuracy:  53.09% || Time:  23.43\n",
      "Saving model with acc:  55.330, loss:   0.01, epoch: 5\n",
      "Saving model with acc:  55.330, loss:   0.01, epoch: 5\n",
      "Training: Epoch 6 || Loss:   0.013 || Accuracy:  56.39% || Time:  23.61\n",
      "Saving model with acc:  59.235, loss:   0.01, epoch: 6\n",
      "Saving model with acc:  59.235, loss:   0.01, epoch: 6\n",
      "Training: Epoch 7 || Loss:   0.012 || Accuracy:  59.37% || Time:  23.22\n",
      "Saving model with acc:  62.227, loss:   0.01, epoch: 7\n",
      "Saving model with acc:  62.227, loss:   0.01, epoch: 7\n",
      "Training: Epoch 8 || Loss:   0.011 || Accuracy:  62.65% || Time:  23.63\n",
      "Saving model with acc:  65.775, loss:   0.01, epoch: 8\n",
      "Saving model with acc:  65.775, loss:   0.01, epoch: 8\n",
      "Training: Epoch 9 || Loss:   0.010 || Accuracy:  65.34% || Time:  23.29\n",
      "Saving model with acc:  65.945, loss:   0.01, epoch: 9\n",
      "Training: Epoch 10 || Loss:   0.010 || Accuracy:  67.93% || Time:  23.49\n",
      "Saving model with acc:  66.880, loss:   0.01, epoch: 10\n",
      "Saving model with acc:  66.880, loss:   0.01, epoch: 10\n",
      "Training: Epoch 11 || Loss:   0.009 || Accuracy:  70.57% || Time:  23.37\n",
      "Saving model with acc:  69.505, loss:   0.01, epoch: 11\n",
      "Saving model with acc:  69.505, loss:   0.01, epoch: 11\n",
      "Training: Epoch 12 || Loss:   0.009 || Accuracy:  72.38% || Time:  23.55\n",
      "Saving model with acc:  72.170, loss:   0.01, epoch: 12\n",
      "Saving model with acc:  72.170, loss:   0.01, epoch: 12\n",
      "Training: Epoch 13 || Loss:   0.008 || Accuracy:  74.11% || Time:  23.47\n",
      "Saving model with acc:  75.860, loss:   0.01, epoch: 13\n",
      "Saving model with acc:  75.860, loss:   0.01, epoch: 13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dd259f8351c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mvariant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_word_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Here\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mvariant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-5acda18a7e5a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-5acda18a7e5a>\u001b[0m in \u001b[0;36mtrain_single_epoch\u001b[0;34m(self, epoch_idx)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vis_lang/experimental_notebooks/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vis_lang/experimental_notebooks/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    788\u001b[0m                         \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset \n",
    "class Cifar10EmbMissingClasses(Cifar10Emb):\n",
    "    def __init__(self, root_path, missing_classes, variant_name=\"cifar10_emb_missingclass\", epochs=200):\n",
    "        super(Cifar10EmbMissingClasses, self).__init__(\n",
    "            root_path=root_path, variant_name=variant_name, epochs=epochs\n",
    "        )\n",
    "        self.missing_classes = missing_classes\n",
    "\n",
    "    def init_datasets(self):\n",
    "        super(Cifar10EmbMissingClasses, self).init_datasets()\n",
    "        \n",
    "        new_datasets = []\n",
    "        \n",
    "        for dataset in [self.train_dataset, self.test_dataset]:\n",
    "            for missing_class in self.missing_classes:\n",
    "                assert missing_class in dataset.classes, f\"{missing_class} not in the selected Dataset\"\n",
    "\n",
    "                mask = np.ones((len(dataset)), dtype=bool)\n",
    "\n",
    "                for c in self.missing_classes:\n",
    "                    # Get index for class\n",
    "                    idx = dataset.class_to_idx[c]\n",
    "\n",
    "                    # get indices where targets == idx \n",
    "                    mask = np.logical_and(mask, np.array(dataset.targets) != idx)\n",
    "\n",
    "                # Use mask on dataset\n",
    "                mask_indices = np.where(mask > 0)[0]\n",
    "                new_datasets.append(Subset(dataset, mask_indices))\n",
    "\n",
    "        self.train_dataset = new_datasets[0]\n",
    "        self.test_dataset = new_datasets[1]\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if os.path.exists(\"/nethome/bdevnani3/raid\"):\n",
    "        root_path = \"/nethome/bdevnani3/raid\"\n",
    "    else:\n",
    "        root_path = \".\"\n",
    "\n",
    "    variant = Cifar10EmbMissingClasses(root_path=root_path, missing_classes=[\"cat\", \"airplane\"])\n",
    "\n",
    "    variant.init_datasets()\n",
    "    variant.init_dataloaders()\n",
    "    variant.set_up_model_architecture(300)\n",
    "    variant.init_model_helpers(nn.MSELoss)\n",
    "    variant.init_word_lookup()\n",
    "    print(\"Here\")\n",
    "    variant.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-ribbon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-chick",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
