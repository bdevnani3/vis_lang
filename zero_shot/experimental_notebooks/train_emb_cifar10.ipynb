{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inside-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, datasets\n",
    "import gensim.downloader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "otherwise-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "twelve-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_cifar10(train=True, train_on_embeddings=False):\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.491, 0.482, 0.446],\n",
    "                std= [0.247, 0.243, 0.261]\n",
    "            )]) # TODO: Automate calculation given a dataset\n",
    "\n",
    "    dataset = datasets.CIFAR10(root='/nethome/bdevnani3/raid/data', train=train,\n",
    "                                            download=True, transform=transform)\n",
    "    if train_on_embeddings:\n",
    "        dataset = change_target_to_word_vectors(dataset)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=4,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def change_target_to_word_vectors(dataset):\n",
    "    model = 'word2vec-google-news-300'\n",
    "    global word_vectors\n",
    "    word_vectors = gensim.downloader.load(model)\n",
    "\n",
    "    def transform_targets(x):\n",
    "        return word_vectors[idx_to_class[x]]\n",
    "\n",
    "    idx_to_class = {y:x for x,y in dataset.class_to_idx.items()}\n",
    "    dataset.targets = np.array(list(map(transform_targets, dataset.targets)))\n",
    "    return dataset\n",
    "\n",
    "def set_up_model(out_features=10, loss=nn.CrossEntropyLoss()):\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    model.fc = nn.Linear(in_features=512, out_features=out_features)\n",
    "    criterion = loss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    return model, criterion, optimizer, lr_scheduler\n",
    "\n",
    "def train_model(model, trainloader, optimizer, criterion, scheduler, epochs=15, verbose=False, emb_model=False):\n",
    "    model.train()\n",
    "    if emb_model:\n",
    "        print(\"########## {} ##########\".format(\"Embedding Model\"))\n",
    "    else:\n",
    "        print(\"########## {} ##########\".format(\"Base Model\"))\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        if verbose:\n",
    "            print(\"########## {} ##########\".format(epoch+1))\n",
    "        train_loss = 0.0\n",
    "        total  = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            if torch.cuda.is_available():\n",
    "                model.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # print statistics\n",
    "            if verbose:\n",
    "                if emb_model:\n",
    "                    # print statistics\n",
    "                    train_loss += loss.item()\n",
    "                    total += labels.size(0)\n",
    "                    labels, outputs = labels.to(\"cpu\"), outputs.to(\"cpu\")\n",
    "                    for l, o in zip(labels, outputs):\n",
    "                        label_word = word_vectors.similar_by_vector(l.numpy(), topn=1)\n",
    "                        output_word = word_vectors.similar_by_vector(o.data.numpy(), topn=1)\n",
    "                        correct += label_word[0][0] == output_word[0][0]\n",
    "                    if i % 200 == 199:    # print every 200 mini-batches\n",
    "                        print(\"Loss: {} | Acc: {} | {}/{}\".format(train_loss/200, 100.*correct/total, correct, total))\n",
    "                        train_loss = 0\n",
    "                        print(label_word, output_word)\n",
    "\n",
    "                else:\n",
    "                    # print statistics\n",
    "                    train_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "                    if i % 200 == 199:    # print every 200 mini-batches\n",
    "                        print(\"Loss: {} | Acc: {} | {}/{}\".format(train_loss/200, 100.*correct/total, correct, total))\n",
    "                        train_loss = 0\n",
    "    if verbose:\n",
    "        print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "julian-barbados",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "########## Base Model ##########\n",
      "########## 1 ##########\n",
      "Loss: 2.3749050003290177 | Acc: 11.125 | 89/800\n",
      "Loss: 2.3867587500810625 | Acc: 10.625 | 170/1600\n",
      "Loss: 2.378164930939674 | Acc: 10.958333333333334 | 263/2400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1f58e636578d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_up_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# torch.save(model.state_dict(), \"/nethome/bdevnani3/raid/trained_models/vis_lang/pred_class.pt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d7eafbe57fe3>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, trainloader, optimizer, criterion, scheduler, epochs, verbose, emb_model)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train base model\n",
    "trainloader = load_data_cifar10()\n",
    "model, criterion, optimizer, scheduler = set_up_model()\n",
    "model = train_model(model, trainloader, optimizer, criterion, scheduler, epochs=20, verbose=True)\n",
    "# torch.save(model.state_dict(), \"/nethome/bdevnani3/raid/trained_models/vis_lang/pred_class.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-population",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
