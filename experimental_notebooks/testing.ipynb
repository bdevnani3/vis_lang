{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "homeless-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path: str = \".\",\n",
    "        project_path: str = \"vis_lang\",\n",
    "        variant_name: str = \"base\",\n",
    "        epochs: int = 200,\n",
    "    ):\n",
    "        self.root_path = root_path\n",
    "        self.project_path = project_path\n",
    "        self.variant_name = variant_name\n",
    "        self.epochs = epochs\n",
    "        self.checkpoints_path = os.path.join(self.root_path, project_path, self.variant_name, \"models\")\n",
    "        make_dirs(self.checkpoints_path)\n",
    "        self.plots_path = os.path.join(self.root_path, project_path, self.variant_name, \"plots\")\n",
    "        make_dirs(self.plots_path)\n",
    "\n",
    "\n",
    "        self.class_names = None\n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "        self.model = None\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.train_losses = []\n",
    "        self.train_accuracy = []\n",
    "        self.test_losses = []\n",
    "        self.test_accuracy = []\n",
    "        self.best_accuracy = 0\n",
    "        self.min_loss = np.inf\n",
    "\n",
    "    def load_data(self):\n",
    "        NotImplementedError\n",
    "\n",
    "    def set_up_model_architecture(self, num_features_in_last_layer: int):\n",
    "        model = models.resnet18()\n",
    "        model.linear = nn.Linear(\n",
    "            in_features=512, out_features=num_features_in_last_layer\n",
    "        )\n",
    "        self.model = model\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "\n",
    "    def init_model_helpers(self, criterion):\n",
    "        self.criterion = criterion()\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            self.optimizer, milestones=[150, 250, 350], gamma=0.1\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.criterion.cuda()\n",
    "\n",
    "    def train_single_epoch(self, epoch_idx):\n",
    "        \"\"\"\n",
    "        Ensure to update self.train_losses & self.train_accuracy\n",
    "        :param epoch_idx: Index of the epoch\n",
    "        :param train_loader: dataloader object for the training dataset\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        train_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.model.train()\n",
    "        for i, data in enumerate(self.train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            device = get_device()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += self.num_correct_preds(outputs, labels)\n",
    "\n",
    "        epoch_loss = train_loss / len(self.train_loader)\n",
    "        epoch_accuracy = correct * 100 / total\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(\n",
    "            f\"Epoch {epoch_idx} || Loss: {epoch_loss:7.3f} || Accuracy: {epoch_accuracy:6.2f}% || Time: {elapsed:6.2f}\"\n",
    "        )\n",
    "\n",
    "        self.train_losses.append(epoch_loss)\n",
    "        self.train_accuracy.append(epoch_accuracy)\n",
    "\n",
    "    def num_correct_preds(self, outputs, labels):\n",
    "        _, predicted = outputs.max(1)\n",
    "        return predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "    def validate_single_epoch(self, epoch_idx):\n",
    "        \"\"\"\n",
    "        Ensure to update self.test_losses & self.test_accuracy\n",
    "        :param epoch_idx: Index of the epoch\n",
    "        :param test_loader: dataloader object for the test dataset\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        test_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.test_loader, 0):\n",
    "                inputs, labels = data\n",
    "                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += self.num_correct_preds(outputs, labels)\n",
    "\n",
    "        epoch_loss = test_loss / len(self.test_loader)\n",
    "        epoch_accuracy = correct * 100 / total\n",
    "\n",
    "        state = {\n",
    "            \"net\": self.model.state_dict(),\n",
    "            \"acc\": epoch_accuracy,\n",
    "            \"epoch\": epoch_idx,\n",
    "            \"loss\": epoch_loss,\n",
    "        }\n",
    "        if self.best_accuracy < epoch_accuracy:\n",
    "            self.best_accuracy = epoch_accuracy\n",
    "            print(f\"Saving model with acc: {epoch_accuracy:7.3f}, loss: {epoch_loss:6.2f}, epoch: {epoch_idx}\")\n",
    "            torch.save(\n",
    "                state,\n",
    "                os.path.join(self.checkpoints_path, \"cifar10_base_best_acc.pth\"),\n",
    "            )\n",
    "\n",
    "        if self.min_loss > epoch_loss:\n",
    "            self.min_loss = epoch_loss\n",
    "            print(f\"Saving model with acc: {epoch_accuracy:7.3f}, loss: {epoch_loss:6.2f}, epoch: {epoch_idx}\")\n",
    "            torch.save(\n",
    "                state,\n",
    "                os.path.join(self.checkpoints_path,\"cifar10_base_best_loss.pth\"),\n",
    "            )\n",
    "        self.test_losses.append(epoch_loss)\n",
    "        self.test_accuracy.append(epoch_accuracy)\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        print(\"Started Training\")\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.train_single_epoch(epoch)\n",
    "            self.validate_single_epoch(epoch)\n",
    "            self.scheduler.step()\n",
    "\n",
    "        print(\"Finished Training\")\n",
    "\n",
    "        print(\"Training Loss: \", self.train_losses)\n",
    "        print(\"Training Accuracy: \", self.train_accuracy)\n",
    "        print(\"Test Loss: \", self.test_losses)\n",
    "        print(\"Test Accuracy: \", self.test_accuracy)\n",
    "\n",
    "        self.export_plots()\n",
    "\n",
    "    def export_plots(self):\n",
    "\n",
    "        print(f\"Saving plots at {self.plots_path}\")\n",
    "\n",
    "        train_losses_fig = plt.figure()\n",
    "        plt.plot(self.train_losses)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Train Loss\")\n",
    "        train_losses_fig.savefig(os.path.join(self.plots_path, \"train_loss.png\"))\n",
    "\n",
    "        test_losses_fig = plt.figure()\n",
    "        plt.plot(self.test_losses)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Test Loss\")\n",
    "        test_losses_fig.savefig(os.path.join(self.plots_path, \"test_loss.png\"))\n",
    "\n",
    "        train_acc_fig = plt.figure()\n",
    "        plt.plot(self.train_accuracy)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Train Acc\")\n",
    "        train_acc_fig.savefig(os.path.join(self.plots_path, \"train_acc.png\"))\n",
    "\n",
    "        test_acc_fig = plt.figure()\n",
    "        plt.plot(self.test_accuracy)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Test Acc\")\n",
    "        test_acc_fig.savefig(os.path.join(self.plots_path, \"test_acc.png\"))\n",
    "\n",
    "\n",
    "# Utils\n",
    "\n",
    "\n",
    "def make_dirs(path: str):\n",
    "    \"\"\" Why is this not how the standard library works? \"\"\"\n",
    "    path = os.path.split(path)[0]\n",
    "    if path != \"\":\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "subject-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "other-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10(Base):\n",
    "    def __init__(self, root_path, variant_name=\"cifar10_base\", epochs=200):\n",
    "        super(Cifar10, self).__init__(root_path=root_path, variant_name=variant_name, epochs=epochs)\n",
    "\n",
    "    def load_data(self):\n",
    "\n",
    "        transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    (0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        transform_test = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        dataset_args = {\"root\": os.path.join(self.root_path, \"data\"),\n",
    "                        \"download\": True}\n",
    "        dataloader_args = {\"batch_size\":128, \"shuffle\":True, \"num_workers\":2}\n",
    "        train_dataset = datasets.CIFAR10(\n",
    "            **dataset_args,\n",
    "            train=True,\n",
    "            transform=transform_train,\n",
    "        )\n",
    "\n",
    "        self.class_names = train_dataset.classes\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, **dataloader_args\n",
    "        )\n",
    "\n",
    "        test_dataset = datasets.CIFAR10(\n",
    "            **dataset_args,\n",
    "            train=False,\n",
    "            transform=transform_test,\n",
    "        )\n",
    "\n",
    "        self.test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, **dataloader_args\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ordinary-military",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Started Training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f7ede56e131a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_up_model_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_model_helpers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-a42ca400a906>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-a42ca400a906>\u001b[0m in \u001b[0;36mtrain_single_epoch\u001b[0;34m(self, epoch_idx)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "root_path = \"/nethome/bdevnani3/raid\"\n",
    "\n",
    "cifar10 = Cifar10(root_path=root_path, epochs=200)\n",
    "cifar10.load_data()\n",
    "cifar10.set_up_model_architecture(10)\n",
    "cifar10.init_model_helpers(nn.CrossEntropyLoss)\n",
    "cifar10.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "broke-referral",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b5ed2dd338ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "import torch\n",
    "from base import get_device\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Cifar10Emb(Cifar10):\n",
    "    def __init__(self, root_path, variant_name=\"cifar10_emb\", epochs=200):\n",
    "        super(Cifar10Emb, self).__init__(root_path=root_path,\n",
    "                                         variant_name=variant_name,\n",
    "                                         epochs=epochs)\n",
    "\n",
    "    def find_closest_words(\n",
    "            word_lookup: torch.Tensor, x: torch.Tensor, mode: str = \"l2\"\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given a size [N, c] lookup table (N classes, c channels per vector) and a set of [M, c] vectors to look up,\n",
    "        returns a size [M] vector of indices from 0 to N-1 containing the closest vector in the lookup for that input.\n",
    "\n",
    "        Modes:\n",
    "            l2     - Computes pairwise L2 distance and chooses the lowest one.\n",
    "            cossim - Computs pairwise cosine similarity, and chooses the most similar. (Not implemented)\n",
    "        \"\"\"\n",
    "        N, c = word_lookup.shape\n",
    "        M, c2 = x.shape\n",
    "\n",
    "        assert c == c2, \"The lookup should have the same number of channels as the input.\"\n",
    "\n",
    "        if mode == \"l2\":\n",
    "            return (\n",
    "                ((word_lookup[None, :, :] - x[:, None, :]) ** 2).sum(dim=-1).argmin(dim=-1)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def init_word_lookup(self):\n",
    "        # We only need to lazily initialize this once. Don't reinitialize it if it's already been initialized.\n",
    "        word_vectors = gensim.downloader.load(name=\"word2vec-google-news-300\")\n",
    "\n",
    "        # Note: we store the word lookup in the model, not the datset because\n",
    "        #   1.) The word lookup should be on the same device as the model\n",
    "        #   2.) If using multiple GPUs, the model will get duplicated to each device, but the dataset won't\n",
    "        #   3.) The word model (i.e., textual feature encoder) is a property of the model not the dataset\n",
    "        self.model.word_lookup = torch.from_numpy(\n",
    "            np.stack([word_vectors[_class] for _class in self.class_names])\n",
    "        ).to(get_device())\n",
    "\n",
    "    def num_correct_preds(self, outputs, labels):\n",
    "        return (self.find_closest_words(self.model.word_lookup, outputs) == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-clothing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
