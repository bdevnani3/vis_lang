{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b7e0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "phrase = \"This is a {} with petals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bca3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_image_features(data_loader):\n",
    "    \"\"\"Given a dataloader object, generate two torch arrays of encoded images and corresponding labels\"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    global clip_model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader):\n",
    "            print(images.shape)\n",
    "            features = clip_model.encode_image(images.to(device))\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features), torch.cat(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f75258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings from the linear probe\n",
    "def get_clip_linear_probe_classifier(\n",
    "    train_features, train_labels, C=1\n",
    "):\n",
    "\n",
    "    classifier = LogisticRegression(C=C, max_iter=1000, n_jobs=6)\n",
    "    classifier.fit(train_features.cpu().numpy(), train_labels.cpu().numpy())\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def get_clip_linear_probe_embedding(classifier, imgs):\n",
    "    return torch.from_numpy(classifier.predict_proba(imgs.cpu().detach().numpy())).to(torch.float16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb08fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for \n",
    "\n",
    "def get_clip_text_features(classes):\n",
    "    \"\"\"Given a dataloader object, generate two torch arrays of encoded images and corresponding labels\"\"\"\n",
    "    # Assumes the positions are in accordance to the label numbers\n",
    "    embedding_per_class = {}\n",
    "    \n",
    "    global clip_model\n",
    "\n",
    "    global phrase\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i,_class in enumerate(classes):\n",
    "            _class = _class.replace(\"_\", \" \")\n",
    "            text = clip.tokenize(phrase.format(_class)).cuda() \n",
    "            class_embeddings = clip_model.encode_text(\n",
    "                    text\n",
    "                )\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            embedding_per_class[i] = class_embeddings\n",
    "    return embedding_per_class\n",
    "\n",
    "def get_text_embeds(classes):\n",
    "    text_clip_features = get_clip_text_features(classes)\n",
    "    text_embeds = []\n",
    "    for c in range(len(classes)):\n",
    "        text_embs = text_clip_features[c].squeeze()\n",
    "        text_embeds.append(text_embs)\n",
    "\n",
    "    text_embeds = torch.stack(text_embeds).squeeze(1)\n",
    "    return text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de544469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FinetuneLambda(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.tensor([1.0]).cuda())\n",
    "        self.b = torch.nn.Parameter(torch.tensor([1.0]).cuda())\n",
    "        self.sftmx = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, image_image, image_text):\n",
    "        out = (self.a*(image_image) + (self.b)*self.sftmx((image_text)*100))/(self.a + self.b)\n",
    "        return out[0]\n",
    "    \n",
    "    def string(self):\n",
    "        return f'A: {self.a.item()}, B: {self.b.item(),}, Lambda: {self.a.item()/(self.a.item()+self.b.item()) }'\n",
    "    \n",
    "model = FinetuneLambda()\n",
    "model.logit_scale = nn.Parameter(torch.ones([], device=device))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "num_epochs = 500\n",
    "\n",
    "def num_correct_preds(outputs, labels):\n",
    "    predicted = outputs.argmax().item()\n",
    "    labels = labels.item()\n",
    "    return predicted == labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "181166aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cb0c1ff06d48c3a21485f4a57405d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "dataset_obj = Flowers102(4, 1)\n",
    "num_elements_per_class = 4\n",
    "\n",
    "train_loader, _, _, _ = dataset_obj.get_train_loaders(transform_fn=clip_preprocess,num_elements_per_class=num_elements_per_class)\n",
    "test_loader = dataset_obj.get_test_loader(transform_fn=clip_preprocess)\n",
    "train_features, train_labels = get_clip_image_features(train_loader)\n",
    "classes = dataset_obj.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2d99fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc = get_clip_linear_probe_classifier(train_features, train_labels)\n",
    "text_embeddings_per_class = get_text_embeds(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0bca375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcff1b9628ef439e810908b94e940db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cea43f0315b4b75b920d75fe00167d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  3.4188034188034186 %\n",
      "----------------\n",
      "Training: Epoch 0 || Loss:   3.997 || A: 1.0234801769256592, B: (0.9759701490402222,) Lambda: 0.5118807722473641\n",
      "Training: Epoch 1 || Loss:   3.995 || A: 1.0464024543762207, B: (0.9513680934906006,) Lambda: 0.5237851040969385\n",
      "Training: Epoch 2 || Loss:   3.992 || A: 1.0687774419784546, B: (0.926176905632019,) Lambda: 0.5357403006533058\n",
      "Training: Epoch 3 || Loss:   3.990 || A: 1.0906144380569458, B: (0.9003782272338867,) Lambda: 0.5477742118641282\n",
      "Training: Epoch 4 || Loss:   3.988 || A: 1.1119189262390137, B: (0.8739504218101501,) Lambda: 0.5599154482802794\n",
      "Training: Epoch 5 || Loss:   3.986 || A: 1.1326944828033447, B: (0.8468690514564514,) Lambda: 0.5721940534870911\n",
      "Training: Epoch 6 || Loss:   3.983 || A: 1.1529446840286255, B: (0.8191070556640625,) Lambda: 0.5846422083267923\n",
      "Training: Epoch 7 || Loss:   3.981 || A: 1.1726678609848022, B: (0.790632963180542,) Lambda: 0.5972940298047994\n",
      "Training: Epoch 8 || Loss:   3.979 || A: 1.1918624639511108, B: (0.7614122629165649,) Lambda: 0.610186804527192\n",
      "Training: Epoch 9 || Loss:   3.976 || A: 1.2105227708816528, B: (0.7314057350158691,) Lambda: 0.6233611418779665\n",
      "Training: Epoch 10 || Loss:   3.974 || A: 1.2286409139633179, B: (0.7005686163902283,) Lambda: 0.6368623493883309\n",
      "Training: Epoch 11 || Loss:   3.971 || A: 1.2462061643600464, B: (0.6688510775566101,) Lambda: 0.6507409476245208\n",
      "Training: Epoch 12 || Loss:   3.969 || A: 1.2632042169570923, B: (0.6361965537071228,) Lambda: 0.665054072035231\n",
      "Training: Epoch 13 || Loss:   3.966 || A: 1.279616355895996, B: (0.6025399565696716,) Lambda: 0.6798672073201346\n",
      "Training: Epoch 14 || Loss:   3.963 || A: 1.295419692993164, B: (0.5678074359893799,) Lambda: 0.6952559206780959\n",
      "Training: Epoch 15 || Loss:   3.960 || A: 1.3105846643447876, B: (0.5319139361381531,) Lambda: 0.7113083635457134\n",
      "Training: Epoch 16 || Loss:   3.957 || A: 1.3250759840011597, B: (0.4947608411312103,) Lambda: 0.7281290089867135\n",
      "Training: Epoch 17 || Loss:   3.954 || A: 1.3388499021530151, B: (0.4562336802482605,) Lambda: 0.7458426533888975\n",
      "Training: Epoch 18 || Loss:   3.951 || A: 1.351853370666504, B: (0.4161973297595978,) Lambda: 0.7646010209665968\n",
      "Training: Epoch 19 || Loss:   3.947 || A: 1.3640185594558716, B: (0.37449172139167786,) Lambda: 0.7845904476278923\n",
      "Training: Epoch 20 || Loss:   3.943 || A: 1.3752636909484863, B: (0.3309248685836792,) Lambda: 0.8060443749110483\n",
      "Training: Epoch 21 || Loss:   3.939 || A: 1.3854835033416748, B: (0.28526270389556885,) Lambda: 0.8292603013791777\n",
      "Training: Epoch 22 || Loss:   3.935 || A: 1.3945447206497192, B: (0.2372160255908966,) Lambda: 0.8546257310470212\n",
      "Training: Epoch 23 || Loss:   3.930 || A: 1.4022732973098755, B: (0.18642091751098633,) Lambda: 0.8826577727973871\n",
      "Training: Epoch 24 || Loss:   3.924 || A: 1.4084398746490479, B: (0.13240861892700195,) Lambda: 0.9140677234140627\n",
      "Training: Epoch 25 || Loss:   3.918 || A: 1.412730097770691, B: (0.07455895096063614,) Lambda: 0.9498692261438786\n",
      "Training: Epoch 26 || Loss:   3.911 || A: 1.4147000312805176, B: (0.012022612616419792,) Lambda: 0.9915732657164665\n",
      "Training: Epoch 27 || Loss:   3.903 || A: 1.4136931896209717, B: (-0.05641607567667961,) Lambda: 1.0415656280483008\n",
      "Training: Epoch 28 || Loss:   3.892 || A: 1.4086763858795166, B: (-0.1326030194759369,) Lambda: 1.1039148868451494\n",
      "Training: Epoch 29 || Loss:   3.879 || A: 1.3978779315948486, B: (-0.2195844203233719,) Lambda: 1.1863579984297987\n",
      "Training: Epoch 30 || Loss:   3.861 || A: 1.3778212070465088, B: (-0.3230687081813812,) Lambda: 1.3062981206766426\n",
      "Training: Epoch 31 || Loss:   3.833 || A: 1.339889645576477, B: (-0.45641961693763733,) Lambda: 1.516621506267555\n",
      "Training: Epoch 32 || Loss:   3.770 || A: 1.2459001541137695, B: (-0.6738107800483704,) Lambda: 2.1778068438155307\n",
      "Training: Epoch 33 || Loss:   3.804 || A: 1.8991845846176147, B: (-0.11466526240110397,) Lambda: 1.0642555454421647\n",
      "Training: Epoch 34 || Loss:   3.890 || A: 1.8948386907577515, B: (-0.1724882870912552,) Lambda: 1.1001470355417031\n",
      "Training: Epoch 35 || Loss:   3.883 || A: 1.8881875276565552, B: (-0.2346324920654297,) Lambda: 1.1418957863604167\n",
      "Training: Epoch 36 || Loss:   3.875 || A: 1.878614902496338, B: (-0.30212974548339844,) Lambda: 1.1916476943277168\n",
      "Training: Epoch 37 || Loss:   3.865 || A: 1.8651949167251587, B: (-0.3764907419681549,) Lambda: 1.252898291246888\n",
      "Training: Epoch 38 || Loss:   3.852 || A: 1.8464332818984985, B: (-0.46008190512657166,) Lambda: 1.3318652924757481\n",
      "Training: Epoch 39 || Loss:   3.835 || A: 1.819663166999817, B: (-0.5569902658462524,) Lambda: 1.4411199965861246\n",
      "Training: Epoch 40 || Loss:   3.811 || A: 1.779280424118042, B: (-0.6754652261734009,) Lambda: 1.611936878049107\n",
      "Training: Epoch 41 || Loss:   3.768 || A: 1.7091922760009766, B: (-0.8379552364349365,) Lambda: 1.9617993707571466\n",
      "Training: Epoch 42 || Loss:   3.614 || A: 1.4526381492614746, B: (-1.2370150089263916,) Lambda: 6.736930679165713\n",
      "Training: Epoch 43 || Loss:   3.380 || A: 1.507470965385437, B: (-1.333810806274414,) Lambda: 8.680580353618664\n",
      "Training: Epoch 44 || Loss:   3.890 || A: 4.3588361740112305, B: (1.520699381828308,) Lambda: 0.7413572267085019\n",
      "Training: Epoch 45 || Loss:   3.953 || A: 4.362964630126953, B: (1.508746862411499,) Lambda: 0.7430481956872783\n",
      "Training: Epoch 46 || Loss:   3.953 || A: 4.367072105407715, B: (1.4967511892318726,) Lambda: 0.744748244613693\n",
      "Training: Epoch 47 || Loss:   3.952 || A: 4.371156215667725, B: (1.484713077545166,) Lambda: 0.746457271635829\n",
      "Training: Epoch 48 || Loss:   3.952 || A: 4.375216007232666, B: (1.4726308584213257,) Lambda: 0.7481755435371452\n",
      "Training: Epoch 49 || Loss:   3.952 || A: 4.379253387451172, B: (1.4605051279067993,) Lambda: 0.7499031639637462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cb71c30b594201bd42b992e0740f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  32.84493284493284 %\n",
      "----------------\n",
      "Training: Epoch 50 || Loss:   3.951 || A: 4.383269309997559, B: (1.4483345746994019,) Lambda: 0.7516404400339917\n",
      "Training: Epoch 51 || Loss:   3.951 || A: 4.387263298034668, B: (1.4361188411712646,) Lambda: 0.7533874977047116\n",
      "Training: Epoch 52 || Loss:   3.951 || A: 4.391234397888184, B: (1.4238578081130981,) Lambda: 0.7551444142805421\n",
      "Training: Epoch 53 || Loss:   3.950 || A: 4.395180702209473, B: (1.4115514755249023,) Lambda: 0.7569112140323216\n",
      "Training: Epoch 54 || Loss:   3.950 || A: 4.39910364151001, B: (1.3991988897323608,) Lambda: 0.7586881881044999\n",
      "Training: Epoch 55 || Loss:   3.950 || A: 4.403006553649902, B: (1.3867998123168945,) Lambda: 0.7604756144404627\n",
      "Training: Epoch 56 || Loss:   3.949 || A: 4.406883239746094, B: (1.3743536472320557,) Lambda: 0.7622734245109908\n",
      "Training: Epoch 57 || Loss:   3.949 || A: 4.410735607147217, B: (1.3618602752685547,) Lambda: 0.7640818267883616\n",
      "Training: Epoch 58 || Loss:   3.949 || A: 4.414565086364746, B: (1.349318504333496,) Lambda: 0.765901152738229\n",
      "Training: Epoch 59 || Loss:   3.948 || A: 4.418369293212891, B: (1.3367284536361694,) Lambda: 0.7677314074521091\n",
      "Training: Epoch 60 || Loss:   3.948 || A: 4.422149181365967, B: (1.3240898847579956,) Lambda: 0.7695727815147899\n",
      "Training: Epoch 61 || Loss:   3.948 || A: 4.425907611846924, B: (1.3114013671875,) Lambda: 0.7714257028896837\n",
      "Training: Epoch 62 || Loss:   3.947 || A: 4.429638385772705, B: (1.2986624240875244,) Lambda: 0.7732901139108979\n",
      "Training: Epoch 63 || Loss:   3.947 || A: 4.433343887329102, B: (1.2858738899230957,) Lambda: 0.7751661258577052\n",
      "Training: Epoch 64 || Loss:   3.947 || A: 4.437025547027588, B: (1.2730340957641602,) Lambda: 0.777054150849158\n",
      "Training: Epoch 65 || Loss:   3.946 || A: 4.440680027008057, B: (1.2601430416107178,) Lambda: 0.7789541919749438\n",
      "Training: Epoch 66 || Loss:   3.946 || A: 4.444309234619141, B: (1.2472004890441895,) Lambda: 0.7808664924424602\n",
      "Training: Epoch 67 || Loss:   3.946 || A: 4.447911262512207, B: (1.2342044115066528,) Lambda: 0.7827913963191597\n",
      "Training: Epoch 68 || Loss:   3.945 || A: 4.451489448547363, B: (1.2211556434631348,) Lambda: 0.7847290596087102\n",
      "Training: Epoch 69 || Loss:   3.945 || A: 4.45504093170166, B: (1.2080531120300293,) Lambda: 0.7866796661504876\n",
      "Training: Epoch 70 || Loss:   3.945 || A: 4.458566188812256, B: (1.1948963403701782,) Lambda: 0.7886434491778659\n",
      "Training: Epoch 71 || Loss:   3.944 || A: 4.462060928344727, B: (1.1816848516464233,) Lambda: 0.7906204677333506\n",
      "Training: Epoch 72 || Loss:   3.944 || A: 4.465528964996338, B: (1.1684179306030273,) Lambda: 0.7926111210746999\n",
      "Training: Epoch 73 || Loss:   3.943 || A: 4.468971252441406, B: (1.1550941467285156,) Lambda: 0.7946158046279119\n",
      "Training: Epoch 74 || Loss:   3.943 || A: 4.472383975982666, B: (1.141714096069336,) Lambda: 0.796634458212799\n",
      "Training: Epoch 75 || Loss:   3.943 || A: 4.475769519805908, B: (1.1282771825790405,) Lambda: 0.7986674197238804\n",
      "Training: Epoch 76 || Loss:   3.942 || A: 4.4791259765625, B: (1.1147822141647339,) Lambda: 0.8007149606043485\n",
      "Training: Epoch 77 || Loss:   3.942 || A: 4.482456207275391, B: (1.1012285947799683,) Lambda: 0.8027774428859944\n",
      "Training: Epoch 78 || Loss:   3.942 || A: 4.485755443572998, B: (1.0876154899597168,) Lambda: 0.8048549965666245\n",
      "Training: Epoch 79 || Loss:   3.941 || A: 4.4890265464782715, B: (1.0739431381225586,) Lambda: 0.806947871548644\n",
      "Training: Epoch 80 || Loss:   3.941 || A: 4.492265701293945, B: (1.0602099895477295,) Lambda: 0.8090563473701553\n",
      "Training: Epoch 81 || Loss:   3.940 || A: 4.495476722717285, B: (1.0464146137237549,) Lambda: 0.811180957872091\n",
      "Training: Epoch 82 || Loss:   3.940 || A: 4.498656272888184, B: (1.0325570106506348,) Lambda: 0.813321787152273\n",
      "Training: Epoch 83 || Loss:   3.940 || A: 4.501806259155273, B: (1.0186364650726318,) Lambda: 0.8154792077450456\n",
      "Training: Epoch 84 || Loss:   3.939 || A: 4.504927635192871, B: (1.004652738571167,) Lambda: 0.8176534925681086\n",
      "Training: Epoch 85 || Loss:   3.939 || A: 4.508012294769287, B: (0.9906039237976074,) Lambda: 0.8198448692504332\n",
      "Training: Epoch 86 || Loss:   3.938 || A: 4.511068820953369, B: (0.976490318775177,) Lambda: 0.822053795884288\n",
      "Training: Epoch 87 || Loss:   3.938 || A: 4.5140910148620605, B: (0.962310791015625,) Lambda: 0.8242804627697696\n",
      "Training: Epoch 88 || Loss:   3.938 || A: 4.517082214355469, B: (0.9480642676353455,) Lambda: 0.8265253693090419\n",
      "Training: Epoch 89 || Loss:   3.937 || A: 4.52003812789917, B: (0.9337498545646667,) Lambda: 0.8287887505772034\n",
      "Training: Epoch 90 || Loss:   3.937 || A: 4.522960662841797, B: (0.9193670749664307,) Lambda: 0.8310709829951026\n",
      "Training: Epoch 91 || Loss:   3.936 || A: 4.525851249694824, B: (0.9049147963523865,) Lambda: 0.8333725318528443\n",
      "Training: Epoch 92 || Loss:   3.936 || A: 4.528707504272461, B: (0.8903917670249939,) Lambda: 0.835693770781982\n",
      "Training: Epoch 93 || Loss:   3.935 || A: 4.531530380249023, B: (0.8757975697517395,) Lambda: 0.8380350557891323\n",
      "Training: Epoch 94 || Loss:   3.935 || A: 4.534315586090088, B: (0.861131489276886,) Lambda: 0.840396638638455\n",
      "Training: Epoch 95 || Loss:   3.935 || A: 4.537067890167236, B: (0.8463922142982483,) Lambda: 0.8427791424336588\n",
      "Training: Epoch 96 || Loss:   3.934 || A: 4.539783954620361, B: (0.8315784931182861,) Lambda: 0.8451829491662061\n",
      "Training: Epoch 97 || Loss:   3.934 || A: 4.5424628257751465, B: (0.8166897296905518,) Lambda: 0.8476084191972432\n",
      "Training: Epoch 98 || Loss:   3.933 || A: 4.545108318328857, B: (0.8017250299453735,) Lambda: 0.8500561027950979\n",
      "Training: Epoch 99 || Loss:   3.933 || A: 4.5477142333984375, B: (0.7866829037666321,) Lambda: 0.8525263711084118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98e7abc99c64f65b20003621d227cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  57.63125763125763 %\n",
      "----------------\n",
      "Training: Epoch 100 || Loss:   3.932 || A: 4.55027961730957, B: (0.7715627551078796,) Lambda: 0.8550196151793581\n",
      "Training: Epoch 101 || Loss:   3.932 || A: 4.552806854248047, B: (0.7563631534576416,) Lambda: 0.857536460056871\n",
      "Training: Epoch 102 || Loss:   3.931 || A: 4.555294990539551, B: (0.7410824298858643,) Lambda: 0.8600774886193177\n",
      "Training: Epoch 103 || Loss:   3.931 || A: 4.557746887207031, B: (0.7257205247879028,) Lambda: 0.8626431341016099\n",
      "Training: Epoch 104 || Loss:   3.930 || A: 4.560157775878906, B: (0.7102754712104797,) Lambda: 0.8652339498649885\n",
      "Training: Epoch 105 || Loss:   3.930 || A: 4.562526702880859, B: (0.694746196269989,) Lambda: 0.8678504598111686\n",
      "Training: Epoch 106 || Loss:   3.929 || A: 4.564853191375732, B: (0.6791316866874695,) Lambda: 0.8704932026923964\n",
      "Training: Epoch 107 || Loss:   3.929 || A: 4.567140102386475, B: (0.6634306907653809,) Lambda: 0.8731628502889245\n",
      "Training: Epoch 108 || Loss:   3.928 || A: 4.569382667541504, B: (0.6476414203643799,) Lambda: 0.8758599903984067\n",
      "Training: Epoch 109 || Loss:   3.928 || A: 4.571582317352295, B: (0.6317631602287292,) Lambda: 0.8785851981286416\n",
      "Training: Epoch 110 || Loss:   3.927 || A: 4.57373571395874, B: (0.6157940030097961,) Lambda: 0.881339150829738\n",
      "Training: Epoch 111 || Loss:   3.927 || A: 4.575843811035156, B: (0.5997330546379089,) Lambda: 0.8841224717160266\n",
      "Training: Epoch 112 || Loss:   3.926 || A: 4.577908039093018, B: (0.5835784673690796,) Lambda: 0.8869359695819317\n",
      "Training: Epoch 113 || Loss:   3.926 || A: 4.579927921295166, B: (0.5673287510871887,) Lambda: 0.8897803651154225\n",
      "Training: Epoch 114 || Loss:   3.925 || A: 4.581902027130127, B: (0.5509822964668274,) Lambda: 0.8926563971188975\n",
      "Training: Epoch 115 || Loss:   3.925 || A: 4.583827972412109, B: (0.5345379114151001,) Lambda: 0.8955647322704869\n",
      "Training: Epoch 116 || Loss:   3.924 || A: 4.585705757141113, B: (0.5179941058158875,) Lambda: 0.8985061583312287\n",
      "Training: Epoch 117 || Loss:   3.924 || A: 4.587531566619873, B: (0.5013486742973328,) Lambda: 0.9014815341366785\n",
      "Training: Epoch 118 || Loss:   3.923 || A: 4.589310646057129, B: (0.48459985852241516,) Lambda: 0.904491839561413\n",
      "Training: Epoch 119 || Loss:   3.923 || A: 4.591036796569824, B: (0.4677463471889496,) Lambda: 0.9075377746195689\n",
      "Training: Epoch 120 || Loss:   3.922 || A: 4.592710494995117, B: (0.4507862329483032,) Lambda: 0.9106202983238338\n",
      "Training: Epoch 121 || Loss:   3.921 || A: 4.594331741333008, B: (0.4337177276611328,) Lambda: 0.9137403618767701\n",
      "Training: Epoch 122 || Loss:   3.921 || A: 4.59589958190918, B: (0.4165385663509369,) Lambda: 0.91689901121363\n",
      "Training: Epoch 123 || Loss:   3.920 || A: 4.597411632537842, B: (0.39924725890159607,) Lambda: 0.9200971554040622\n",
      "Training: Epoch 124 || Loss:   3.920 || A: 4.598871231079102, B: (0.3818414509296417,) Lambda: 0.9233359811520702\n",
      "Training: Epoch 125 || Loss:   3.919 || A: 4.600271224975586, B: (0.36431899666786194,) Lambda: 0.9266165019864903\n",
      "Training: Epoch 126 || Loss:   3.918 || A: 4.601612567901611, B: (0.3466780483722687,) Lambda: 0.9299398367524903\n",
      "Training: Epoch 127 || Loss:   3.918 || A: 4.602893352508545, B: (0.3289162218570709,) Lambda: 0.9333071934555828\n",
      "Training: Epoch 128 || Loss:   3.917 || A: 4.604114532470703, B: (0.3110310733318329,) Lambda: 0.9367198658439237\n",
      "Training: Epoch 129 || Loss:   3.917 || A: 4.605276107788086, B: (0.2930205464363098,) Lambda: 0.9401790934439215\n",
      "Training: Epoch 130 || Loss:   3.916 || A: 4.606370449066162, B: (0.27488183975219727,) Lambda: 0.9436862052015058\n",
      "Training: Epoch 131 || Loss:   3.915 || A: 4.6074018478393555, B: (0.25661224126815796,) Lambda: 0.947242701898661\n",
      "Training: Epoch 132 || Loss:   3.915 || A: 4.608368873596191, B: (0.23820994794368744,) Lambda: 0.9508498764355179\n",
      "Training: Epoch 133 || Loss:   3.914 || A: 4.609266757965088, B: (0.21967175602912903,) Lambda: 0.9545093077096504\n",
      "Training: Epoch 134 || Loss:   3.913 || A: 4.610097885131836, B: (0.2009950876235962,) Lambda: 0.9582225725501868\n",
      "Training: Epoch 135 || Loss:   3.913 || A: 4.610855579376221, B: (0.18217690289020538,) Lambda: 0.9619913064298572\n",
      "Training: Epoch 136 || Loss:   3.912 || A: 4.611544132232666, B: (0.1632142812013626,) Lambda: 0.9658172692586601\n",
      "Training: Epoch 137 || Loss:   3.911 || A: 4.612159252166748, B: (0.14410407841205597,) Lambda: 0.9697022497712465\n",
      "Training: Epoch 138 || Loss:   3.910 || A: 4.6127028465271, B: (0.12484320998191833,) Lambda: 0.973648127428673\n",
      "Training: Epoch 139 || Loss:   3.910 || A: 4.613166332244873, B: (0.1054283082485199,) Lambda: 0.9776568414366918\n",
      "Training: Epoch 140 || Loss:   3.909 || A: 4.613552570343018, B: (0.08585592359304428,) Lambda: 0.9817304829525185\n",
      "Training: Epoch 141 || Loss:   3.908 || A: 4.613856315612793, B: (0.06612252444028854,) Lambda: 0.9858711915801015\n",
      "Training: Epoch 142 || Loss:   3.907 || A: 4.614077091217041, B: (0.046224359422922134,) Lambda: 0.9900812511996248\n",
      "Training: Epoch 143 || Loss:   3.907 || A: 4.614211559295654, B: (0.02615763060748577,) Lambda: 0.99436302812621\n",
      "Training: Epoch 144 || Loss:   3.906 || A: 4.614257335662842, B: (0.0059183519333601,) Lambda: 0.9987190201556081\n",
      "Training: Epoch 145 || Loss:   3.905 || A: 4.6142168045043945, B: (-0.014497647061944008,) Lambda: 1.0031518548341123\n",
      "Training: Epoch 146 || Loss:   3.904 || A: 4.614086627960205, B: (-0.03509466350078583,) Lambda: 1.0076642771538318\n",
      "Training: Epoch 147 || Loss:   3.903 || A: 4.6138596534729, B: (-0.0558771975338459,) Lambda: 1.0122591953948918\n",
      "Training: Epoch 148 || Loss:   3.903 || A: 4.613536834716797, B: (-0.07684994488954544,) Lambda: 1.0169396625237392\n",
      "Training: Epoch 149 || Loss:   3.902 || A: 4.613112926483154, B: (-0.09801777452230453,) Lambda: 1.0217089056206792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330f3bc9010943a59474fcbe6cc6cc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  64.59096459096459 %\n",
      "----------------\n",
      "Training: Epoch 150 || Loss:   3.901 || A: 4.612587928771973, B: (-0.11938578635454178,) Lambda: 1.0265703127904033\n",
      "Training: Epoch 151 || Loss:   3.900 || A: 4.611958980560303, B: (-0.1409592181444168,) Lambda: 1.031527449258519\n",
      "Training: Epoch 152 || Loss:   3.899 || A: 4.611219882965088, B: (-0.16274376213550568,) Lambda: 1.0365841599943568\n",
      "Training: Epoch 153 || Loss:   3.898 || A: 4.610368728637695, B: (-0.18474513292312622,) Lambda: 1.0417444296668201\n",
      "Training: Epoch 154 || Loss:   3.897 || A: 4.609404563903809, B: (-0.20696939527988434,) Lambda: 1.0470124799917444\n",
      "Training: Epoch 155 || Loss:   3.896 || A: 4.608321666717529, B: (-0.22942295670509338,) Lambda: 1.0523928439314008\n",
      "Training: Epoch 156 || Loss:   3.895 || A: 4.607112407684326, B: (-0.2521124482154846,) Lambda: 1.0578903445606997\n",
      "Training: Epoch 157 || Loss:   3.894 || A: 4.605778694152832, B: (-0.2750447988510132,) Lambda: 1.0635099744062766\n",
      "Training: Epoch 158 || Loss:   3.893 || A: 4.604313850402832, B: (-0.2982273995876312,) Lambda: 1.0692571788778584\n",
      "Training: Epoch 159 || Loss:   3.892 || A: 4.6027140617370605, B: (-0.3216680586338043,) Lambda: 1.0751377253130736\n",
      "Training: Epoch 160 || Loss:   3.891 || A: 4.6009745597839355, B: (-0.34537455439567566,) Lambda: 1.0811576637744096\n",
      "Training: Epoch 161 || Loss:   3.890 || A: 4.599091529846191, B: (-0.36935585737228394,) Lambda: 1.0873236263381567\n",
      "Training: Epoch 162 || Loss:   3.889 || A: 4.597060203552246, B: (-0.3936206102371216,) Lambda: 1.0936425043107816\n",
      "Training: Epoch 163 || Loss:   3.887 || A: 4.594871997833252, B: (-0.41817834973335266,) Lambda: 1.1001218631209866\n",
      "Training: Epoch 164 || Loss:   3.886 || A: 4.5925211906433105, B: (-0.4430391490459442,) Lambda: 1.1067697473093276\n",
      "Training: Epoch 165 || Loss:   3.885 || A: 4.590001583099365, B: (-0.4682135581970215,) Lambda: 1.1135947689129682\n",
      "Training: Epoch 166 || Loss:   3.884 || A: 4.587307453155518, B: (-0.49371275305747986,) Lambda: 1.120606163830937\n",
      "Training: Epoch 167 || Loss:   3.882 || A: 4.584434986114502, B: (-0.5195486545562744,) Lambda: 1.1278138211449351\n",
      "Training: Epoch 168 || Loss:   3.881 || A: 4.581369876861572, B: (-0.5457336902618408,) Lambda: 1.135228664088686\n",
      "Training: Epoch 169 || Loss:   3.880 || A: 4.578110218048096, B: (-0.5722816586494446,) Lambda: 1.1428622443930438\n",
      "Training: Epoch 170 || Loss:   3.878 || A: 4.574641704559326, B: (-0.5992068648338318,) Lambda: 1.1507273767503652\n",
      "Training: Epoch 171 || Loss:   3.877 || A: 4.570962429046631, B: (-0.6265242099761963,) Lambda: 1.1588373743432203\n",
      "Training: Epoch 172 || Loss:   3.875 || A: 4.567057132720947, B: (-0.6542499661445618,) Lambda: 1.1672073113475243\n",
      "Training: Epoch 173 || Loss:   3.874 || A: 4.562918663024902, B: (-0.6824013590812683,) Lambda: 1.1758531931780765\n",
      "Training: Epoch 174 || Loss:   3.872 || A: 4.558528900146484, B: (-0.710997462272644,) Lambda: 1.184793152116658\n",
      "Training: Epoch 175 || Loss:   3.870 || A: 4.553882598876953, B: (-0.7400580644607544,) Lambda: 1.1940461753765603\n",
      "Training: Epoch 176 || Loss:   3.869 || A: 4.548962593078613, B: (-0.7696048617362976,) Lambda: 1.2036337696624861\n",
      "Training: Epoch 177 || Loss:   3.867 || A: 4.543756008148193, B: (-0.7996611595153809,) Lambda: 1.2135793006973057\n",
      "Training: Epoch 178 || Loss:   3.865 || A: 4.5382466316223145, B: (-0.830251932144165,) Lambda: 1.223908608138251\n",
      "Training: Epoch 179 || Loss:   3.863 || A: 4.5324201583862305, B: (-0.8614039421081543,) Lambda: 1.2346499964474424\n",
      "Training: Epoch 180 || Loss:   3.861 || A: 4.526252269744873, B: (-0.8931471109390259,) Lambda: 1.2458357443285764\n",
      "Training: Epoch 181 || Loss:   3.859 || A: 4.519725322723389, B: (-0.9255133271217346,) Lambda: 1.2575010400761872\n",
      "Training: Epoch 182 || Loss:   3.857 || A: 4.512820720672607, B: (-0.9585380554199219,) Lambda: 1.269685375558552\n",
      "Training: Epoch 183 || Loss:   3.855 || A: 4.505510330200195, B: (-0.9922598600387573,) Lambda: 1.2824335664269226\n",
      "Training: Epoch 184 || Loss:   3.852 || A: 4.497766971588135, B: (-1.026721477508545,) Lambda: 1.2957960301182392\n",
      "Training: Epoch 185 || Loss:   3.850 || A: 4.4895548820495605, B: (-1.0619676113128662,) Lambda: 1.3098294886258626\n",
      "Training: Epoch 186 || Loss:   3.847 || A: 4.48084831237793, B: (-1.0980515480041504,) Lambda: 1.3245987342687495\n",
      "Training: Epoch 187 || Loss:   3.844 || A: 4.471609592437744, B: (-1.13503098487854,) Lambda: 1.3401781040935359\n",
      "Training: Epoch 188 || Loss:   3.841 || A: 4.461792469024658, B: (-1.1729692220687866,) Lambda: 1.3566531655827003\n",
      "Training: Epoch 189 || Loss:   3.838 || A: 4.451350688934326, B: (-1.2119383811950684,) Lambda: 1.3741229167709323\n",
      "Training: Epoch 190 || Loss:   3.835 || A: 4.440229415893555, B: (-1.252020001411438,) Lambda: 1.3927031881043523\n",
      "Training: Epoch 191 || Loss:   3.831 || A: 4.428368091583252, B: (-1.2933061122894287,) Lambda: 1.4125296791040627\n",
      "Training: Epoch 192 || Loss:   3.828 || A: 4.415688514709473, B: (-1.3359028100967407,) Lambda: 1.4337648584107328\n",
      "Training: Epoch 193 || Loss:   3.824 || A: 4.402112007141113, B: (-1.379931092262268,) Lambda: 1.4566010874691753\n",
      "Training: Epoch 194 || Loss:   3.819 || A: 4.387545585632324, B: (-1.425532579421997,) Lambda: 1.4812715462197983\n",
      "Training: Epoch 195 || Loss:   3.815 || A: 4.371872425079346, B: (-1.4728716611862183,) Lambda: 1.508061839627896\n",
      "Training: Epoch 196 || Loss:   3.810 || A: 4.354949951171875, B: (-1.5221441984176636,) Lambda: 1.5373274171509113\n",
      "Training: Epoch 197 || Loss:   3.804 || A: 4.336620330810547, B: (-1.5735838413238525,) Lambda: 1.5695125081812389\n",
      "Training: Epoch 198 || Loss:   3.798 || A: 4.316675662994385, B: (-1.6274744272232056,) Lambda: 1.6051887845263824\n",
      "Training: Epoch 199 || Loss:   3.791 || A: 4.294870853424072, B: (-1.684165596961975,) Lambda: 1.6450998605810736\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f7eed354134525990a0bf9a1b29074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  64.34676434676435 %\n",
      "----------------\n",
      "Training: Epoch 200 || Loss:   3.783 || A: 4.270881652832031, B: (-1.7440961599349976,) Lambda: 1.6902430637019925\n",
      "Training: Epoch 201 || Loss:   3.775 || A: 4.244297504425049, B: (-1.8078278303146362,) Lambda: 1.7419865921272746\n",
      "Training: Epoch 202 || Loss:   3.765 || A: 4.214570045471191, B: (-1.8761000633239746,) Lambda: 1.802276735492372\n",
      "Training: Epoch 203 || Loss:   3.753 || A: 4.1809401512146, B: (-1.94991934299469,) Lambda: 1.874003207773976\n",
      "Training: Epoch 204 || Loss:   3.739 || A: 4.142318248748779, B: (-2.030712842941284,) Lambda: 1.9616914397719696\n",
      "Training: Epoch 205 || Loss:   3.721 || A: 4.097052097320557, B: (-2.1206154823303223,) Lambda: 2.0729488951209296\n",
      "Training: Epoch 206 || Loss:   3.698 || A: 4.042426109313965, B: (-2.223069906234741,) Lambda: 2.221899209441362\n",
      "Training: Epoch 207 || Loss:   3.666 || A: 3.9734480381011963, B: (-2.3442792892456055,) Lambda: 2.4389419701871544\n",
      "Training: Epoch 208 || Loss:   3.618 || A: 3.8790535926818848, B: (-2.4976370334625244,) Lambda: 2.8080259837582524\n",
      "Training: Epoch 209 || Loss:   3.521 || A: 3.723496198654175, B: (-2.7247567176818848,) Lambda: 3.7281956602229114\n",
      "Training: Epoch 210 || Loss:   3.180 || A: 3.4219982624053955, B: (-3.100172281265259,) Lambda: 10.633070239643928\n",
      "Training: Epoch 211 || Loss:   4.060 || A: 10.215635299682617, B: (3.7253289222717285,) Lambda: 0.7327782452518564\n",
      "Training: Epoch 212 || Loss:   3.955 || A: 10.217392921447754, B: (3.720336675643921,) Lambda: 0.7330744114579302\n",
      "Training: Epoch 213 || Loss:   3.955 || A: 10.21915054321289, B: (3.715341329574585,) Lambda: 0.7333708782858286\n",
      "Training: Epoch 214 || Loss:   3.955 || A: 10.220903396606445, B: (3.7103431224823,) Lambda: 0.7336675424271369\n",
      "Training: Epoch 215 || Loss:   3.954 || A: 10.22265625, B: (3.705341339111328,) Lambda: 0.7339645332787751\n",
      "Training: Epoch 216 || Loss:   3.954 || A: 10.224407196044922, B: (3.7003371715545654,) Lambda: 0.7342617520387217\n",
      "Training: Epoch 217 || Loss:   3.954 || A: 10.226153373718262, B: (3.695328712463379,) Lambda: 0.7345592452306975\n",
      "Training: Epoch 218 || Loss:   3.954 || A: 10.227897644042969, B: (3.6903183460235596,) Lambda: 0.7348569422505478\n",
      "Training: Epoch 219 || Loss:   3.954 || A: 10.229641914367676, B: (3.6853044033050537,) Lambda: 0.7351549679624334\n",
      "Training: Epoch 220 || Loss:   3.954 || A: 10.231386184692383, B: (3.6802871227264404,) Lambda: 0.7354533102237374\n",
      "Training: Epoch 221 || Loss:   3.954 || A: 10.23313045501709, B: (3.6752665042877197,) Lambda: 0.7357519694727334\n",
      "Training: Epoch 222 || Loss:   3.954 || A: 10.234872817993164, B: (3.6702427864074707,) Lambda: 0.7360508973225705\n",
      "Training: Epoch 223 || Loss:   3.954 || A: 10.236615180969238, B: (3.6652159690856934,) Lambda: 0.736350130459525\n",
      "Training: Epoch 224 || Loss:   3.954 || A: 10.23835563659668, B: (3.6601858139038086,) Lambda: 0.7366496457963216\n",
      "Training: Epoch 225 || Loss:   3.954 || A: 10.240093231201172, B: (3.655153274536133,) Lambda: 0.7369493752394438\n",
      "Training: Epoch 226 || Loss:   3.954 || A: 10.241830825805664, B: (3.6501173973083496,) Lambda: 0.7372494240055453\n",
      "Training: Epoch 227 || Loss:   3.954 || A: 10.24356746673584, B: (3.645078659057617,) Lambda: 0.7375497491949112\n",
      "Training: Epoch 228 || Loss:   3.954 || A: 10.245305061340332, B: (3.6400363445281982,) Lambda: 0.737850425270079\n",
      "Training: Epoch 229 || Loss:   3.954 || A: 10.247038841247559, B: (3.634991407394409,) Lambda: 0.7381513119991935\n",
      "Training: Epoch 230 || Loss:   3.954 || A: 10.248771667480469, B: (3.629941701889038,) Lambda: 0.7384525780393761\n",
      "Training: Epoch 231 || Loss:   3.954 || A: 10.250502586364746, B: (3.624889373779297,) Lambda: 0.7387540918345584\n",
      "Training: Epoch 232 || Loss:   3.954 || A: 10.25223159790039, B: (3.6198337078094482,) Lambda: 0.7390558919644431\n",
      "Training: Epoch 233 || Loss:   3.953 || A: 10.25395679473877, B: (3.614774465560913,) Lambda: 0.7393579558421118\n",
      "Training: Epoch 234 || Loss:   3.953 || A: 10.255680084228516, B: (3.609712839126587,) Lambda: 0.7396602563605446\n",
      "Training: Epoch 235 || Loss:   3.953 || A: 10.257400512695312, B: (3.6046478748321533,) Lambda: 0.7399628269891573\n",
      "Training: Epoch 236 || Loss:   3.953 || A: 10.25912094116211, B: (3.5995802879333496,) Lambda: 0.7402656837441405\n",
      "Training: Epoch 237 || Loss:   3.953 || A: 10.26084041595459, B: (3.5945098400115967,) Lambda: 0.7405688218914724\n",
      "Training: Epoch 238 || Loss:   3.953 || A: 10.262557029724121, B: (3.589434862136841,) Lambda: 0.740872295467781\n",
      "Training: Epoch 239 || Loss:   3.953 || A: 10.264273643493652, B: (3.5843582153320312,) Lambda: 0.7411760055526545\n",
      "Training: Epoch 240 || Loss:   3.953 || A: 10.265989303588867, B: (3.5792770385742188,) Lambda: 0.7414800878424259\n",
      "Training: Epoch 241 || Loss:   3.953 || A: 10.267704010009766, B: (3.5741915702819824,) Lambda: 0.7417845301931798\n",
      "Training: Epoch 242 || Loss:   3.953 || A: 10.26941967010498, B: (3.5691051483154297,) Lambda: 0.7420891897693743\n",
      "Training: Epoch 243 || Loss:   3.953 || A: 10.27113151550293, B: (3.5640132427215576,) Lambda: 0.7423942210215847\n",
      "Training: Epoch 244 || Loss:   3.953 || A: 10.27284049987793, B: (3.5589191913604736,) Lambda: 0.7426994633506511\n",
      "Training: Epoch 245 || Loss:   3.953 || A: 10.27454948425293, B: (3.5538222789764404,) Lambda: 0.7430050088451984\n",
      "Training: Epoch 246 || Loss:   3.953 || A: 10.27625846862793, B: (3.5487220287323,) Lambda: 0.7433108835553222\n",
      "Training: Epoch 247 || Loss:   3.953 || A: 10.277965545654297, B: (3.5436179637908936,) Lambda: 0.7436170782190544\n",
      "Training: Epoch 248 || Loss:   3.953 || A: 10.279670715332031, B: (3.53851056098938,) Lambda: 0.7439235677814627\n",
      "Training: Epoch 249 || Loss:   3.953 || A: 10.281373977661133, B: (3.5333993434906006,) Lambda: 0.7442303784977325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48291c1401744f6a9ee4dce3acfda11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  31.257631257631257 %\n",
      "----------------\n",
      "Training: Epoch 250 || Loss:   3.953 || A: 10.283077239990234, B: (3.528285503387451,) Lambda: 0.7445374820034175\n",
      "Training: Epoch 251 || Loss:   3.952 || A: 10.284773826599121, B: (3.5231692790985107,) Lambda: 0.7448447424696637\n",
      "Training: Epoch 252 || Loss:   3.952 || A: 10.286467552185059, B: (3.5180487632751465,) Lambda: 0.7451523339984647\n",
      "Training: Epoch 253 || Loss:   3.952 || A: 10.288159370422363, B: (3.5129239559173584,) Lambda: 0.7454602749037206\n",
      "Training: Epoch 254 || Loss:   3.952 || A: 10.289850234985352, B: (3.5077967643737793,) Lambda: 0.7457684803404009\n",
      "Training: Epoch 255 || Loss:   3.952 || A: 10.291542053222656, B: (3.5026652812957764,) Lambda: 0.7460770890016453\n",
      "Training: Epoch 256 || Loss:   3.952 || A: 10.293233871459961, B: (3.4975311756134033,) Lambda: 0.7463859935489482\n",
      "Training: Epoch 257 || Loss:   3.952 || A: 10.294923782348633, B: (3.4923927783966064,) Lambda: 0.7466952497239365\n",
      "Training: Epoch 258 || Loss:   3.952 || A: 10.296610832214355, B: (3.4872522354125977,) Lambda: 0.7470047244155504\n",
      "Training: Epoch 259 || Loss:   3.952 || A: 10.298295021057129, B: (3.4821081161499023,) Lambda: 0.7473144957023626\n",
      "Training: Epoch 260 || Loss:   3.952 || A: 10.299978256225586, B: (3.476961374282837,) Lambda: 0.7476245474297311\n",
      "Training: Epoch 261 || Loss:   3.952 || A: 10.301657676696777, B: (3.4718105792999268,) Lambda: 0.747934905372264\n",
      "Training: Epoch 262 || Loss:   3.952 || A: 10.303338050842285, B: (3.46665620803833,) Lambda: 0.7482456315620759\n",
      "Training: Epoch 263 || Loss:   3.952 || A: 10.305018424987793, B: (3.461498498916626,) Lambda: 0.7485566960727721\n",
      "Training: Epoch 264 || Loss:   3.952 || A: 10.306694030761719, B: (3.456336736679077,) Lambda: 0.7488680512975577\n",
      "Training: Epoch 265 || Loss:   3.952 || A: 10.308367729187012, B: (3.4511735439300537,) Lambda: 0.7491796074137412\n",
      "Training: Epoch 266 || Loss:   3.952 || A: 10.310039520263672, B: (3.4460060596466064,) Lambda: 0.7494915206824225\n",
      "Training: Epoch 267 || Loss:   3.952 || A: 10.311704635620117, B: (3.44083571434021,) Lambda: 0.7498036270549727\n",
      "Training: Epoch 268 || Loss:   3.951 || A: 10.313369750976562, B: (3.435662031173706,) Lambda: 0.7501160746726859\n",
      "Training: Epoch 269 || Loss:   3.951 || A: 10.315030097961426, B: (3.4304850101470947,) Lambda: 0.7504287774473114\n",
      "Training: Epoch 270 || Loss:   3.951 || A: 10.316689491271973, B: (3.425304651260376,) Lambda: 0.7507418053207547\n",
      "Training: Epoch 271 || Loss:   3.951 || A: 10.31834888458252, B: (3.4201197624206543,) Lambda: 0.751055241286539\n",
      "Training: Epoch 272 || Loss:   3.951 || A: 10.320006370544434, B: (3.4149327278137207,) Lambda: 0.7513689210153153\n",
      "Training: Epoch 273 || Loss:   3.951 || A: 10.321661949157715, B: (3.4097416400909424,) Lambda: 0.751682949384673\n",
      "Training: Epoch 274 || Loss:   3.951 || A: 10.32331657409668, B: (3.404547691345215,) Lambda: 0.7519972790002215\n",
      "Training: Epoch 275 || Loss:   3.951 || A: 10.324967384338379, B: (3.399348735809326,) Lambda: 0.7523119763454748\n",
      "Training: Epoch 276 || Loss:   3.951 || A: 10.326617240905762, B: (3.3941478729248047,) Lambda: 0.752626923880251\n",
      "Training: Epoch 277 || Loss:   3.951 || A: 10.328266143798828, B: (3.388942003250122,) Lambda: 0.7529422921252965\n",
      "Training: Epoch 278 || Loss:   3.951 || A: 10.329915046691895, B: (3.383732557296753,) Lambda: 0.753258020403515\n",
      "Training: Epoch 279 || Loss:   3.951 || A: 10.331564903259277, B: (3.3785195350646973,) Lambda: 0.7535741263838844\n",
      "Training: Epoch 280 || Loss:   3.951 || A: 10.333215713500977, B: (3.3733034133911133,) Lambda: 0.7538905843152609\n",
      "Training: Epoch 281 || Loss:   3.951 || A: 10.334863662719727, B: (3.368083953857422,) Lambda: 0.7542073393185215\n",
      "Training: Epoch 282 || Loss:   3.951 || A: 10.336507797241211, B: (3.362861394882202,) Lambda: 0.754524361835893\n",
      "Training: Epoch 283 || Loss:   3.951 || A: 10.338149070739746, B: (3.357635736465454,) Lambda: 0.754841669628232\n",
      "Training: Epoch 284 || Loss:   3.951 || A: 10.339788436889648, B: (3.3524067401885986,) Lambda: 0.7551592935367459\n",
      "Training: Epoch 285 || Loss:   3.950 || A: 10.341426849365234, B: (3.3471734523773193,) Lambda: 0.7554773038444825\n",
      "Training: Epoch 286 || Loss:   3.950 || A: 10.343062400817871, B: (3.3419368267059326,) Lambda: 0.7557956145160389\n",
      "Training: Epoch 287 || Loss:   3.950 || A: 10.34469223022461, B: (3.3366966247558594,) Lambda: 0.756114188396802\n",
      "Training: Epoch 288 || Loss:   3.950 || A: 10.346320152282715, B: (3.3314530849456787,) Lambda: 0.7564330810896854\n",
      "Training: Epoch 289 || Loss:   3.950 || A: 10.34794807434082, B: (3.326205015182495,) Lambda: 0.7567523931166952\n",
      "Training: Epoch 290 || Loss:   3.950 || A: 10.34957504272461, B: (3.320955276489258,) Lambda: 0.7570719497383601\n",
      "Training: Epoch 291 || Loss:   3.950 || A: 10.351202964782715, B: (3.3157012462615967,) Lambda: 0.7573919305308251\n",
      "Training: Epoch 292 || Loss:   3.950 || A: 10.352829933166504, B: (3.310443639755249,) Lambda: 0.7577122625784222\n",
      "Training: Epoch 293 || Loss:   3.950 || A: 10.354455947875977, B: (3.305182456970215,) Lambda: 0.7580329464799305\n",
      "Training: Epoch 294 || Loss:   3.950 || A: 10.356080055236816, B: (3.2999167442321777,) Lambda: 0.758354018920062\n",
      "Training: Epoch 295 || Loss:   3.950 || A: 10.35770320892334, B: (3.2946479320526123,) Lambda: 0.7586754180264154\n",
      "Training: Epoch 296 || Loss:   3.950 || A: 10.359326362609863, B: (3.2893757820129395,) Lambda: 0.7589971744449813\n",
      "Training: Epoch 297 || Loss:   3.950 || A: 10.360946655273438, B: (3.284100294113159,) Lambda: 0.7593192382338565\n",
      "Training: Epoch 298 || Loss:   3.950 || A: 10.362564086914062, B: (3.2788209915161133,) Lambda: 0.7596416366325879\n",
      "Training: Epoch 299 || Loss:   3.950 || A: 10.364175796508789, B: (3.2735378742218018,) Lambda: 0.7599643200276668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325605f3d07345fd936fc5200a52fd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  34.065934065934066 %\n",
      "----------------\n",
      "Training: Epoch 300 || Loss:   3.950 || A: 10.365784645080566, B: (3.2682511806488037,) Lambda: 0.7602873263336196\n",
      "Training: Epoch 301 || Loss:   3.950 || A: 10.367389678955078, B: (3.2629618644714355,) Lambda: 0.7606105863025184\n",
      "Training: Epoch 302 || Loss:   3.949 || A: 10.368993759155273, B: (3.257668972015381,) Lambda: 0.760934204046634\n",
      "Training: Epoch 303 || Loss:   3.949 || A: 10.370596885681152, B: (3.2523715496063232,) Lambda: 0.7612582334712213\n",
      "Training: Epoch 304 || Loss:   3.949 || A: 10.372199058532715, B: (3.2470712661743164,) Lambda: 0.7615825819769706\n",
      "Training: Epoch 305 || Loss:   3.949 || A: 10.373796463012695, B: (3.241767168045044,) Lambda: 0.7619072367558534\n",
      "Training: Epoch 306 || Loss:   3.949 || A: 10.375391006469727, B: (3.236459255218506,) Lambda: 0.7622322319892242\n",
      "Training: Epoch 307 || Loss:   3.949 || A: 10.376985549926758, B: (3.231147527694702,) Lambda: 0.7625576183548414\n",
      "Training: Epoch 308 || Loss:   3.949 || A: 10.378578186035156, B: (3.225831985473633,) Lambda: 0.762883363203105\n",
      "Training: Epoch 309 || Loss:   3.949 || A: 10.380172729492188, B: (3.220512628555298,) Lambda: 0.7632095336541456\n",
      "Training: Epoch 310 || Loss:   3.949 || A: 10.38176441192627, B: (3.215190887451172,) Lambda: 0.7635359669382465\n",
      "Training: Epoch 311 || Loss:   3.949 || A: 10.383355140686035, B: (3.209864377975464,) Lambda: 0.7638628307614108\n",
      "Training: Epoch 312 || Loss:   3.949 || A: 10.3849458694458, B: (3.2045347690582275,) Lambda: 0.7641900485895985\n",
      "Training: Epoch 313 || Loss:   3.949 || A: 10.38653564453125, B: (3.1992013454437256,) Lambda: 0.7645176446589212\n",
      "Training: Epoch 314 || Loss:   3.949 || A: 10.38812255859375, B: (3.1938633918762207,) Lambda: 0.7648456268822966\n",
      "Training: Epoch 315 || Loss:   3.949 || A: 10.389705657958984, B: (3.188521146774292,) Lambda: 0.7651739661866014\n",
      "Training: Epoch 316 || Loss:   3.949 || A: 10.391284942626953, B: (3.183175802230835,) Lambda: 0.7655025962311858\n",
      "Training: Epoch 317 || Loss:   3.949 || A: 10.392861366271973, B: (3.177828311920166,) Lambda: 0.7658314803980169\n",
      "Training: Epoch 318 || Loss:   3.949 || A: 10.394436836242676, B: (3.172476053237915,) Lambda: 0.7661608002438222\n",
      "Training: Epoch 319 || Loss:   3.948 || A: 10.396008491516113, B: (3.1671204566955566,) Lambda: 0.7664904264503695\n",
      "Training: Epoch 320 || Loss:   3.948 || A: 10.397577285766602, B: (3.1617610454559326,) Lambda: 0.7668204031626326\n",
      "Training: Epoch 321 || Loss:   3.948 || A: 10.399144172668457, B: (3.1563973426818848,) Lambda: 0.7671507745295479\n",
      "Training: Epoch 322 || Loss:   3.948 || A: 10.40070915222168, B: (3.1510305404663086,) Lambda: 0.7674814738238747\n",
      "Training: Epoch 323 || Loss:   3.948 || A: 10.402271270751953, B: (3.145659923553467,) Lambda: 0.7678125258802851\n",
      "Training: Epoch 324 || Loss:   3.948 || A: 10.40383243560791, B: (3.1402852535247803,) Lambda: 0.7681439776586975\n",
      "Training: Epoch 325 || Loss:   3.948 || A: 10.40539264678955, B: (3.1349070072174072,) Lambda: 0.7684758028017719\n",
      "Training: Epoch 326 || Loss:   3.948 || A: 10.406951904296875, B: (3.1295242309570312,) Lambda: 0.7688080561227739\n",
      "Training: Epoch 327 || Loss:   3.948 || A: 10.408509254455566, B: (3.124138355255127,) Lambda: 0.7691406408149339\n",
      "Training: Epoch 328 || Loss:   3.948 || A: 10.410065650939941, B: (3.118748188018799,) Lambda: 0.7694736415813644\n",
      "Training: Epoch 329 || Loss:   3.948 || A: 10.411620140075684, B: (3.1133551597595215,) Lambda: 0.7698069615108682\n",
      "Training: Epoch 330 || Loss:   3.948 || A: 10.413172721862793, B: (3.1079585552215576,) Lambda: 0.7701406419676633\n",
      "Training: Epoch 331 || Loss:   3.948 || A: 10.414722442626953, B: (3.1025588512420654,) Lambda: 0.7704746402925504\n",
      "Training: Epoch 332 || Loss:   3.948 || A: 10.41627025604248, B: (3.097154140472412,) Lambda: 0.7708090821693451\n",
      "Training: Epoch 333 || Loss:   3.948 || A: 10.417817115783691, B: (3.0917465686798096,) Lambda: 0.7711438621637031\n",
      "Training: Epoch 334 || Loss:   3.948 || A: 10.419364929199219, B: (3.0863335132598877,) Lambda: 0.771479162931915\n",
      "Training: Epoch 335 || Loss:   3.947 || A: 10.420912742614746, B: (3.0809178352355957,) Lambda: 0.7718148055945969\n",
      "Training: Epoch 336 || Loss:   3.947 || A: 10.422457695007324, B: (3.0754973888397217,) Lambda: 0.7721508650951018\n",
      "Training: Epoch 337 || Loss:   3.947 || A: 10.424001693725586, B: (3.070073366165161,) Lambda: 0.7724873062778104\n",
      "Training: Epoch 338 || Loss:   3.947 || A: 10.425541877746582, B: (3.064646005630493,) Lambda: 0.772824067972632\n",
      "Training: Epoch 339 || Loss:   3.947 || A: 10.427080154418945, B: (3.0592145919799805,) Lambda: 0.7731612240792198\n",
      "Training: Epoch 340 || Loss:   3.947 || A: 10.42861557006836, B: (3.0537805557250977,) Lambda: 0.7734986772949917\n",
      "Training: Epoch 341 || Loss:   3.947 || A: 10.43015193939209, B: (3.0483412742614746,) Lambda: 0.7738366428694315\n",
      "Training: Epoch 342 || Loss:   3.947 || A: 10.431686401367188, B: (3.042897939682007,) Lambda: 0.7741750051308023\n",
      "Training: Epoch 343 || Loss:   3.947 || A: 10.433218002319336, B: (3.0374507904052734,) Lambda: 0.7745137352017909\n",
      "Training: Epoch 344 || Loss:   3.947 || A: 10.434749603271484, B: (3.03200101852417,) Lambda: 0.774852813148783\n",
      "Training: Epoch 345 || Loss:   3.947 || A: 10.436279296875, B: (3.0265464782714844,) Lambda: 0.7751923311776978\n",
      "Training: Epoch 346 || Loss:   3.947 || A: 10.437804222106934, B: (3.0210864543914795,) Lambda: 0.7755322836772256\n",
      "Training: Epoch 347 || Loss:   3.947 || A: 10.439326286315918, B: (3.0156242847442627,) Lambda: 0.7758725111015664\n",
      "Training: Epoch 348 || Loss:   3.947 || A: 10.44084644317627, B: (3.0101590156555176,) Lambda: 0.7762130849720918\n",
      "Training: Epoch 349 || Loss:   3.947 || A: 10.442361831665039, B: (3.0046892166137695,) Lambda: 0.7765540410439385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd2a784bb88484dade1ff2bff403d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  37.60683760683761 %\n",
      "----------------\n",
      "Training: Epoch 350 || Loss:   3.947 || A: 10.443876266479492, B: (2.9992144107818604,) Lambda: 0.7768954712285803\n",
      "Training: Epoch 351 || Loss:   3.946 || A: 10.445387840270996, B: (2.9937362670898438,) Lambda: 0.7772372482630677\n",
      "Training: Epoch 352 || Loss:   3.946 || A: 10.44689655303955, B: (2.9882540702819824,) Lambda: 0.7775794143241838\n",
      "Training: Epoch 353 || Loss:   3.946 || A: 10.448404312133789, B: (2.9827675819396973,) Lambda: 0.7779220156317227\n",
      "Training: Epoch 354 || Loss:   3.946 || A: 10.449911117553711, B: (2.9772775173187256,) Lambda: 0.7782650115165369\n",
      "Training: Epoch 355 || Loss:   3.946 || A: 10.45141887664795, B: (2.97178316116333,) Lambda: 0.778608475623608\n",
      "Training: Epoch 356 || Loss:   3.946 || A: 10.452921867370605, B: (2.966284990310669,) Lambda: 0.778952286691017\n",
      "Training: Epoch 357 || Loss:   3.946 || A: 10.454425811767578, B: (2.9607818126678467,) Lambda: 0.7792966090755937\n",
      "Training: Epoch 358 || Loss:   3.946 || A: 10.45592975616455, B: (2.955275058746338,) Lambda: 0.7796413447164274\n",
      "Training: Epoch 359 || Loss:   3.946 || A: 10.457433700561523, B: (2.949765205383301,) Lambda: 0.7799864665187178\n",
      "Training: Epoch 360 || Loss:   3.946 || A: 10.45893383026123, B: (2.9442505836486816,) Lambda: 0.7803320097130709\n",
      "Training: Epoch 361 || Loss:   3.946 || A: 10.460430145263672, B: (2.9387331008911133,) Lambda: 0.7806778642140617\n",
      "Training: Epoch 362 || Loss:   3.946 || A: 10.461919784545898, B: (2.933210611343384,) Lambda: 0.7810241091610776\n",
      "Training: Epoch 363 || Loss:   3.946 || A: 10.463406562805176, B: (2.927684783935547,) Lambda: 0.781370710711482\n",
      "Training: Epoch 364 || Loss:   3.946 || A: 10.464896202087402, B: (2.922154426574707,) Lambda: 0.7817178325808166\n",
      "Training: Epoch 365 || Loss:   3.946 || A: 10.46638298034668, B: (2.9166200160980225,) Lambda: 0.7820653543249714\n",
      "Training: Epoch 366 || Loss:   3.945 || A: 10.467866897583008, B: (2.911080837249756,) Lambda: 0.7824133186744866\n",
      "Training: Epoch 367 || Loss:   3.945 || A: 10.469350814819336, B: (2.9055376052856445,) Lambda: 0.7827617312367202\n",
      "Training: Epoch 368 || Loss:   3.945 || A: 10.470830917358398, B: (2.899991273880005,) Lambda: 0.7831104750027785\n",
      "Training: Epoch 369 || Loss:   3.945 || A: 10.472310066223145, B: (2.8944406509399414,) Lambda: 0.7834596670361001\n",
      "Training: Epoch 370 || Loss:   3.945 || A: 10.473784446716309, B: (2.888885259628296,) Lambda: 0.7838092744104382\n",
      "Training: Epoch 371 || Loss:   3.945 || A: 10.475259780883789, B: (2.8833253383636475,) Lambda: 0.7841593767135362\n",
      "Training: Epoch 372 || Loss:   3.945 || A: 10.476733207702637, B: (2.8777637481689453,) Lambda: 0.7845097604441269\n",
      "Training: Epoch 373 || Loss:   3.945 || A: 10.478205680847168, B: (2.872197389602661,) Lambda: 0.7848606237245326\n",
      "Training: Epoch 374 || Loss:   3.945 || A: 10.47967529296875, B: (2.866626262664795,) Lambda: 0.7852119367515132\n",
      "Training: Epoch 375 || Loss:   3.945 || A: 10.481143951416016, B: (2.8610496520996094,) Lambda: 0.7855637733104298\n",
      "Training: Epoch 376 || Loss:   3.945 || A: 10.482608795166016, B: (2.855469226837158,) Lambda: 0.785915990135413\n",
      "Training: Epoch 377 || Loss:   3.945 || A: 10.48406982421875, B: (2.849886894226074,) Lambda: 0.7862684757118017\n",
      "Training: Epoch 378 || Loss:   3.945 || A: 10.485530853271484, B: (2.8442983627319336,) Lambda: 0.7866215450594709\n",
      "Training: Epoch 379 || Loss:   3.945 || A: 10.486989974975586, B: (2.8387057781219482,) Lambda: 0.7869750420001825\n",
      "Training: Epoch 380 || Loss:   3.945 || A: 10.488447189331055, B: (2.8331100940704346,) Lambda: 0.7873289110425207\n",
      "Training: Epoch 381 || Loss:   3.945 || A: 10.489903450012207, B: (2.8275105953216553,) Lambda: 0.7876832104418684\n",
      "Training: Epoch 382 || Loss:   3.944 || A: 10.491357803344727, B: (2.8219070434570312,) Lambda: 0.788037939909613\n",
      "Training: Epoch 383 || Loss:   3.944 || A: 10.492809295654297, B: (2.816298484802246,) Lambda: 0.7883931416546363\n",
      "Training: Epoch 384 || Loss:   3.944 || A: 10.49425983428955, B: (2.810685873031616,) Lambda: 0.7887487904978815\n",
      "Training: Epoch 385 || Loss:   3.944 || A: 10.495706558227539, B: (2.805068016052246,) Lambda: 0.7891049126209151\n",
      "Training: Epoch 386 || Loss:   3.944 || A: 10.497151374816895, B: (2.799445390701294,) Lambda: 0.7894615110867285\n",
      "Training: Epoch 387 || Loss:   3.944 || A: 10.498587608337402, B: (2.7938191890716553,) Lambda: 0.7898184104916031\n",
      "Training: Epoch 388 || Loss:   3.944 || A: 10.500021934509277, B: (2.788188934326172,) Lambda: 0.790175745866191\n",
      "Training: Epoch 389 || Loss:   3.944 || A: 10.501452445983887, B: (2.782554864883423,) Lambda: 0.7905334738406019\n",
      "Training: Epoch 390 || Loss:   3.944 || A: 10.50288200378418, B: (2.77691650390625,) Lambda: 0.7908916688534003\n",
      "Training: Epoch 391 || Loss:   3.944 || A: 10.504310607910156, B: (2.771272897720337,) Lambda: 0.7912503886141823\n",
      "Training: Epoch 392 || Loss:   3.944 || A: 10.50574016571045, B: (2.765625476837158,) Lambda: 0.7916095787482003\n",
      "Training: Epoch 393 || Loss:   3.944 || A: 10.507170677185059, B: (2.7599754333496094,) Lambda: 0.7919691687756364\n",
      "Training: Epoch 394 || Loss:   3.944 || A: 10.508600234985352, B: (2.754319190979004,) Lambda: 0.7923293429961604\n",
      "Training: Epoch 395 || Loss:   3.944 || A: 10.510028839111328, B: (2.7486588954925537,) Lambda: 0.7926899742635297\n",
      "Training: Epoch 396 || Loss:   3.944 || A: 10.511451721191406, B: (2.742995500564575,) Lambda: 0.7930509319119552\n",
      "Training: Epoch 397 || Loss:   3.943 || A: 10.512873649597168, B: (2.7373275756835938,) Lambda: 0.7934123769787812\n",
      "Training: Epoch 398 || Loss:   3.943 || A: 10.51429557800293, B: (2.7316548824310303,) Lambda: 0.7937743395167781\n",
      "Training: Epoch 399 || Loss:   3.943 || A: 10.515715599060059, B: (2.7259774208068848,) Lambda: 0.794136790762555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75696e706204996bff100dcf93660b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  42.857142857142854 %\n",
      "----------------\n",
      "Training: Epoch 400 || Loss:   3.943 || A: 10.517131805419922, B: (2.7202954292297363,) Lambda: 0.7944996878162834\n",
      "Training: Epoch 401 || Loss:   3.943 || A: 10.518545150756836, B: (2.7146103382110596,) Lambda: 0.7948629606540819\n",
      "Training: Epoch 402 || Loss:   3.943 || A: 10.5199556350708, B: (2.7089195251464844,) Lambda: 0.7952267677834832\n",
      "Training: Epoch 403 || Loss:   3.943 || A: 10.521363258361816, B: (2.703223943710327,) Lambda: 0.7955910530585966\n",
      "Training: Epoch 404 || Loss:   3.943 || A: 10.522767066955566, B: (2.6975247859954834,) Lambda: 0.7959557310837023\n",
      "Training: Epoch 405 || Loss:   3.943 || A: 10.524166107177734, B: (2.6918208599090576,) Lambda: 0.7963208599847449\n",
      "Training: Epoch 406 || Loss:   3.943 || A: 10.525562286376953, B: (2.686115026473999,) Lambda: 0.7966862978206996\n",
      "Training: Epoch 407 || Loss:   3.943 || A: 10.526955604553223, B: (2.6804039478302,) Lambda: 0.7970522467265995\n",
      "Training: Epoch 408 || Loss:   3.943 || A: 10.52834701538086, B: (2.6746881008148193,) Lambda: 0.7974186937112757\n",
      "Training: Epoch 409 || Loss:   3.943 || A: 10.529735565185547, B: (2.668966770172119,) Lambda: 0.7977856684423974\n",
      "Training: Epoch 410 || Loss:   3.943 || A: 10.531126022338867, B: (2.663241386413574,) Lambda: 0.7981531585480239\n",
      "Training: Epoch 411 || Loss:   3.943 || A: 10.532513618469238, B: (2.6575114727020264,) Lambda: 0.7985211207459468\n",
      "Training: Epoch 412 || Loss:   3.942 || A: 10.53390121459961, B: (2.65177845954895,) Lambda: 0.7988895130868418\n",
      "Training: Epoch 413 || Loss:   3.942 || A: 10.535289764404297, B: (2.6460413932800293,) Lambda: 0.7992584086063671\n",
      "Training: Epoch 414 || Loss:   3.942 || A: 10.536678314208984, B: (2.640298843383789,) Lambda: 0.7996278803699368\n",
      "Training: Epoch 415 || Loss:   3.942 || A: 10.538064956665039, B: (2.6345505714416504,) Lambda: 0.7999979149303907\n",
      "Training: Epoch 416 || Loss:   3.942 || A: 10.539447784423828, B: (2.6287975311279297,) Lambda: 0.8003684266100884\n",
      "Training: Epoch 417 || Loss:   3.942 || A: 10.540824890136719, B: (2.6230409145355225,) Lambda: 0.8007393152242156\n",
      "Training: Epoch 418 || Loss:   3.942 || A: 10.542197227478027, B: (2.6172804832458496,) Lambda: 0.8011106108631512\n",
      "Training: Epoch 419 || Loss:   3.942 || A: 10.543566703796387, B: (2.6115152835845947,) Lambda: 0.8014824015471973\n",
      "Training: Epoch 420 || Loss:   3.942 || A: 10.544937133789062, B: (2.605746030807495,) Lambda: 0.8018547022847816\n",
      "Training: Epoch 421 || Loss:   3.942 || A: 10.546304702758789, B: (2.5999722480773926,) Lambda: 0.8022274855610722\n",
      "Training: Epoch 422 || Loss:   3.942 || A: 10.547672271728516, B: (2.5941927433013916,) Lambda: 0.8026008682683545\n",
      "Training: Epoch 423 || Loss:   3.942 || A: 10.549036026000977, B: (2.588409185409546,) Lambda: 0.80297469228177\n",
      "Training: Epoch 424 || Loss:   3.942 || A: 10.550396919250488, B: (2.5826196670532227,) Lambda: 0.8033490896717049\n",
      "Training: Epoch 425 || Loss:   3.942 || A: 10.551758766174316, B: (2.576827049255371,) Lambda: 0.803723943653787\n",
      "Training: Epoch 426 || Loss:   3.941 || A: 10.553119659423828, B: (2.5710299015045166,) Lambda: 0.8040993140493704\n",
      "Training: Epoch 427 || Loss:   3.941 || A: 10.554478645324707, B: (2.5652284622192383,) Lambda: 0.8044751730208817\n",
      "Training: Epoch 428 || Loss:   3.941 || A: 10.555834770202637, B: (2.559422731399536,) Lambda: 0.8048515074075462\n",
      "Training: Epoch 429 || Loss:   3.941 || A: 10.557188034057617, B: (2.5536117553710938,) Lambda: 0.8052283768813188\n",
      "Training: Epoch 430 || Loss:   3.941 || A: 10.558536529541016, B: (2.5477957725524902,) Lambda: 0.805605739742649\n",
      "Training: Epoch 431 || Loss:   3.941 || A: 10.559880256652832, B: (2.541975736618042,) Lambda: 0.8059835386739403\n",
      "Training: Epoch 432 || Loss:   3.941 || A: 10.561223030090332, B: (2.536151170730591,) Lambda: 0.8063618606413774\n",
      "Training: Epoch 433 || Loss:   3.941 || A: 10.5625638961792, B: (2.530320882797241,) Lambda: 0.8067407660334537\n",
      "Training: Epoch 434 || Loss:   3.941 || A: 10.563901901245117, B: (2.5244860649108887,) Lambda: 0.8071201685464465\n",
      "Training: Epoch 435 || Loss:   3.941 || A: 10.565237998962402, B: (2.518646717071533,) Lambda: 0.8075000833670598\n",
      "Training: Epoch 436 || Loss:   3.941 || A: 10.566574096679688, B: (2.512803554534912,) Lambda: 0.8078804954223824\n",
      "Training: Epoch 437 || Loss:   3.941 || A: 10.56790828704834, B: (2.5069544315338135,) Lambda: 0.8082615102359049\n",
      "Training: Epoch 438 || Loss:   3.941 || A: 10.569239616394043, B: (2.501103162765503,) Lambda: 0.8086428791482445\n",
      "Training: Epoch 439 || Loss:   3.941 || A: 10.570569038391113, B: (2.4952452182769775,) Lambda: 0.8090248973956187\n",
      "Training: Epoch 440 || Loss:   3.941 || A: 10.571894645690918, B: (2.4893813133239746,) Lambda: 0.8094074942497633\n",
      "Training: Epoch 441 || Loss:   3.940 || A: 10.573219299316406, B: (2.4835140705108643,) Lambda: 0.8097905501961155\n",
      "Training: Epoch 442 || Loss:   3.940 || A: 10.57453727722168, B: (2.477642774581909,) Lambda: 0.8101740272699087\n",
      "Training: Epoch 443 || Loss:   3.940 || A: 10.575857162475586, B: (2.471766233444214,) Lambda: 0.8105581255344039\n",
      "Training: Epoch 444 || Loss:   3.940 || A: 10.57717514038086, B: (2.4658849239349365,) Lambda: 0.8109427609950756\n",
      "Training: Epoch 445 || Loss:   3.940 || A: 10.578487396240234, B: (2.4599997997283936,) Lambda: 0.8113278202636115\n",
      "Training: Epoch 446 || Loss:   3.940 || A: 10.579798698425293, B: (2.4541077613830566,) Lambda: 0.8117135665388885\n",
      "Training: Epoch 447 || Loss:   3.940 || A: 10.581106185913086, B: (2.448212146759033,) Lambda: 0.8120997519402121\n",
      "Training: Epoch 448 || Loss:   3.940 || A: 10.582412719726562, B: (2.4423117637634277,) Lambda: 0.8124864931416187\n",
      "Training: Epoch 449 || Loss:   3.940 || A: 10.583720207214355, B: (2.4364068508148193,) Lambda: 0.8128738037688849\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71adced4467f4980b9968c290313537a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  48.35164835164835 %\n",
      "----------------\n",
      "Training: Epoch 450 || Loss:   3.940 || A: 10.5850248336792, B: (2.4304964542388916,) Lambda: 0.8132616896032395\n",
      "Training: Epoch 451 || Loss:   3.940 || A: 10.586326599121094, B: (2.4245810508728027,) Lambda: 0.8136501221823721\n",
      "Training: Epoch 452 || Loss:   3.940 || A: 10.587626457214355, B: (2.418661594390869,) Lambda: 0.8140390567397621\n",
      "Training: Epoch 453 || Loss:   3.940 || A: 10.588926315307617, B: (2.412738084793091,) Lambda: 0.8144285215687922\n",
      "Training: Epoch 454 || Loss:   3.940 || A: 10.590221405029297, B: (2.4068076610565186,) Lambda: 0.8148186290252444\n",
      "Training: Epoch 455 || Loss:   3.939 || A: 10.591513633728027, B: (2.400871992111206,) Lambda: 0.8152093032601837\n",
      "Training: Epoch 456 || Loss:   3.939 || A: 10.592804908752441, B: (2.394932508468628,) Lambda: 0.8156004828606197\n",
      "Training: Epoch 457 || Loss:   3.939 || A: 10.594094276428223, B: (2.3889880180358887,) Lambda: 0.8159922302075728\n",
      "Training: Epoch 458 || Loss:   3.939 || A: 10.595380783081055, B: (2.3830387592315674,) Lambda: 0.8163845180484176\n",
      "Training: Epoch 459 || Loss:   3.939 || A: 10.59666633605957, B: (2.377084732055664,) Lambda: 0.8167773745946402\n",
      "Training: Epoch 460 || Loss:   3.939 || A: 10.59794807434082, B: (2.3711256980895996,) Lambda: 0.8171707756701851\n",
      "Training: Epoch 461 || Loss:   3.939 || A: 10.599227905273438, B: (2.365161418914795,) Lambda: 0.817564764542977\n",
      "Training: Epoch 462 || Loss:   3.939 || A: 10.600502014160156, B: (2.359192132949829,) Lambda: 0.8179592738709864\n",
      "Training: Epoch 463 || Loss:   3.939 || A: 10.60177230834961, B: (2.3532183170318604,) Lambda: 0.8183543018223861\n",
      "Training: Epoch 464 || Loss:   3.939 || A: 10.60304069519043, B: (2.3472392559051514,) Lambda: 0.8187499216411475\n",
      "Training: Epoch 465 || Loss:   3.939 || A: 10.604307174682617, B: (2.3412559032440186,) Lambda: 0.8191460742842408\n",
      "Training: Epoch 466 || Loss:   3.939 || A: 10.605573654174805, B: (2.335265874862671,) Lambda: 0.8195429384915366\n",
      "Training: Epoch 467 || Loss:   3.939 || A: 10.606836318969727, B: (2.3292717933654785,) Lambda: 0.8199402963288158\n",
      "Training: Epoch 468 || Loss:   3.938 || A: 10.608098030090332, B: (2.3232734203338623,) Lambda: 0.8203382039375529\n",
      "Training: Epoch 469 || Loss:   3.938 || A: 10.609358787536621, B: (2.3172695636749268,) Lambda: 0.8207367380947608\n",
      "Training: Epoch 470 || Loss:   3.938 || A: 10.610617637634277, B: (2.3112590312957764,) Lambda: 0.8211359626382233\n",
      "Training: Epoch 471 || Loss:   3.938 || A: 10.611873626708984, B: (2.3052451610565186,) Lambda: 0.8215356536598595\n",
      "Training: Epoch 472 || Loss:   3.938 || A: 10.61312484741211, B: (2.2992258071899414,) Lambda: 0.8219359225370416\n",
      "Training: Epoch 473 || Loss:   3.938 || A: 10.614374160766602, B: (2.2932019233703613,) Lambda: 0.822336749485549\n",
      "Training: Epoch 474 || Loss:   3.938 || A: 10.615618705749512, B: (2.287172555923462,) Lambda: 0.822738157229794\n",
      "Training: Epoch 475 || Loss:   3.938 || A: 10.616860389709473, B: (2.2811381816864014,) Lambda: 0.8231401430958968\n",
      "Training: Epoch 476 || Loss:   3.938 || A: 10.618101119995117, B: (2.2750985622406006,) Lambda: 0.8235427497973806\n",
      "Training: Epoch 477 || Loss:   3.938 || A: 10.619339942932129, B: (2.2690534591674805,) Lambda: 0.8239459808235031\n",
      "Training: Epoch 478 || Loss:   3.938 || A: 10.620574951171875, B: (2.2630040645599365,) Lambda: 0.8243497352873266\n",
      "Training: Epoch 479 || Loss:   3.938 || A: 10.621807098388672, B: (2.256950616836548,) Lambda: 0.824754012246974\n",
      "Training: Epoch 480 || Loss:   3.938 || A: 10.623039245605469, B: (2.250890016555786,) Lambda: 0.8251590504562155\n",
      "Training: Epoch 481 || Loss:   3.938 || A: 10.624268531799316, B: (2.2448248863220215,) Lambda: 0.8255646444246788\n",
      "Training: Epoch 482 || Loss:   3.937 || A: 10.625494003295898, B: (2.238754987716675,) Lambda: 0.8259707978848396\n",
      "Training: Epoch 483 || Loss:   3.937 || A: 10.626718521118164, B: (2.2326786518096924,) Lambda: 0.8263776581603669\n",
      "Training: Epoch 484 || Loss:   3.937 || A: 10.627937316894531, B: (2.2265989780426025,) Lambda: 0.8267849631480237\n",
      "Training: Epoch 485 || Loss:   3.937 || A: 10.629151344299316, B: (2.2205142974853516,) Lambda: 0.8271928344762014\n",
      "Training: Epoch 486 || Loss:   3.937 || A: 10.630365371704102, B: (2.2144229412078857,) Lambda: 0.8276014452506097\n",
      "Training: Epoch 487 || Loss:   3.937 || A: 10.63157844543457, B: (2.208327054977417,) Lambda: 0.8280106458021393\n",
      "Training: Epoch 488 || Loss:   3.937 || A: 10.632789611816406, B: (2.202225685119629,) Lambda: 0.8284204861333245\n",
      "Training: Epoch 489 || Loss:   3.937 || A: 10.633997917175293, B: (2.1961190700531006,) Lambda: 0.8288309395589141\n",
      "Training: Epoch 490 || Loss:   3.937 || A: 10.635202407836914, B: (2.190007209777832,) Lambda: 0.8292419948622147\n",
      "Training: Epoch 491 || Loss:   3.937 || A: 10.63640308380127, B: (2.1838903427124023,) Lambda: 0.8296536381768069\n",
      "Training: Epoch 492 || Loss:   3.937 || A: 10.637598991394043, B: (2.177767515182495,) Lambda: 0.8300659201541433\n",
      "Training: Epoch 493 || Loss:   3.937 || A: 10.6387939453125, B: (2.1716408729553223,) Lambda: 0.8304787539406127\n",
      "Training: Epoch 494 || Loss:   3.937 || A: 10.639982223510742, B: (2.165508985519409,) Lambda: 0.8308921578898637\n",
      "Training: Epoch 495 || Loss:   3.936 || A: 10.641169548034668, B: (2.1593704223632812,) Lambda: 0.8313063021281164\n",
      "Training: Epoch 496 || Loss:   3.936 || A: 10.642354965209961, B: (2.1532251834869385,) Lambda: 0.8317211757134574\n",
      "Training: Epoch 497 || Loss:   3.936 || A: 10.643536567687988, B: (2.1470766067504883,) Lambda: 0.8321365381417887\n",
      "Training: Epoch 498 || Loss:   3.936 || A: 10.644716262817383, B: (2.1409225463867188,) Lambda: 0.8325525553838173\n",
      "Training: Epoch 499 || Loss:   3.936 || A: 10.645892143249512, B: (2.134762763977051,) Lambda: 0.8329692195374125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce2b0e796d643f3a18dac668aa29658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  54.09035409035409 %\n",
      "----------------\n",
      "Training: Epoch 500 || Loss:   3.936 || A: 10.647069931030273, B: (2.1285979747772217,) Lambda: 0.8333865602588484\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for epoch in tqdm(range(num_epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    \n",
    "\n",
    "    for inputs, labels in zip(train_features, train_labels):\n",
    "        \n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        image_only = get_clip_linear_probe_embedding(lpc, inputs.repeat(len(classes), 1))\n",
    "        \n",
    "        inputs_norm = inputs/inputs.norm(dim=-1, keepdim=True)\n",
    "        image_text =inputs_norm.repeat(len(classes), 1) @ text_embeddings_per_class.T\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(image_only, image_text)\n",
    "        \n",
    "        loss = criterion(outputs.unsqueeze(0), labels.unsqueeze(0))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss+=loss\n",
    "        total += 1\n",
    "        correct += num_correct_preds(outputs, labels)\n",
    "        \n",
    "    if epoch%50 == 0:\n",
    "\n",
    "        model.eval()\n",
    "        eval_total = 0\n",
    "        eval_correct = 0\n",
    "        for images, target in tqdm(test_loader):\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "            image_features = clip_model.encode_image(images)\n",
    "            image_features_norm = image_features/image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            image_only = get_clip_linear_probe_embedding(lpc, image_features.repeat(len(classes), 1))\n",
    "            image_text = inputs_norm.repeat(len(classes), 1) @ text_embeddings_per_class.T\n",
    "\n",
    "            outputs = model(image_only, image_text)\n",
    "\n",
    "            eval_correct += num_correct_preds(outputs, target)\n",
    "            eval_total+=1\n",
    "        print(\"----------------\")\n",
    "        print(\"Accuracy: \", (eval_correct)*100/eval_total, \"%\" )\n",
    "        print(\"----------------\")\n",
    "        \n",
    "        \n",
    "    epoch_loss = running_loss/len(train_loader)\n",
    "    epoch_accuracy = correct*100/total\n",
    "    print(\n",
    "        f\"Training: Epoch {epoch} || Loss: {epoch_loss:7.3f} || {model.string()}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f25c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72596a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('p3': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd06250aeecf667241d2415373c88b70d4f824c8a2781323a01ee2a69a098796f4a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
