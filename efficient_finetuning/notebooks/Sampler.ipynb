{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5182235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from torch.utils.data import DataLoader\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef27f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Sampler, Dataset\n",
    "# torch.multiprocessing.set_start_method('spawn')# good solution !!!!\n",
    "\n",
    "import clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "class CLIPTaskSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Samples batches in the shape of few-shot classification tasks. At each iteration, it will sample\n",
    "    n_way classes, and then sample support and query images from these classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset: Dataset, n_way: int, n_shot: int, n_query: int, n_tasks: int, phrase=\"This is a photo of a {}\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: dataset from which to sample classification tasks. Must have a field 'label': a\n",
    "                list of length len(dataset) containing containing the labels of all images.\n",
    "            n_way: number of classes in one task\n",
    "            n_shot: number of support images for each class in one task\n",
    "            n_query: number of query images for each class in one task\n",
    "            n_tasks: number of tasks to sample\n",
    "        \"\"\"\n",
    "        super().__init__(data_source=None)\n",
    "        self.n_way = randint(1,n_way+1)\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_tasks = n_tasks\n",
    "        self.classes = dataset.classes\n",
    "        self.phrase = phrase\n",
    "\n",
    "        self.items_per_label = {}\n",
    "        self.n_way_hist = []\n",
    "        assert hasattr(\n",
    "            dataset, \"labels\"\n",
    "        ), \"TaskSampler needs a dataset with a field 'label' containing the labels of all images.\"\n",
    "        for item, label in enumerate(dataset.labels):\n",
    "            if label in self.items_per_label.keys():\n",
    "                self.items_per_label[label].append(item)\n",
    "            else:\n",
    "                self.items_per_label[label] = [item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_tasks\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.n_tasks):\n",
    "            n_way = randint(1,self.n_way+1)\n",
    "            print(n_way)\n",
    "            self.n_way_hist.append(n_way)\n",
    "            yield torch.cat(\n",
    "                [\n",
    "                    # pylint: disable=not-callable\n",
    "                    torch.tensor(\n",
    "                        random.sample(\n",
    "                            self.items_per_label[label], self.n_shot + self.n_query\n",
    "                        )\n",
    "                    )\n",
    "                    # pylint: enable=not-callable\n",
    "                    for label in random.sample(self.items_per_label.keys(), self.n_way)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def episodic_collate_fn(\n",
    "        self, input_data: List[Tuple[torch.Tensor, int]]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, List[int]]:\n",
    "        \"\"\"\n",
    "        Collate function to be used as argument for the collate_fn parameter of episodic\n",
    "            data loaders.\n",
    "        Args:\n",
    "            input_data: each element is a tuple containing:\n",
    "                - an image as a torch Tensor\n",
    "                - the label of this image\n",
    "        Returns:\n",
    "            tuple(Tensor, Tensor, Tensor, Tensor, list[int]): respectively:\n",
    "                - support images,\n",
    "                - their labels,\n",
    "                - query images,\n",
    "                - their labels,\n",
    "                - the dataset class ids of the class sampled in the episode\n",
    "        \"\"\"\n",
    "        \n",
    "        print(self.n_way)\n",
    "        new_input_data  = []\n",
    "\n",
    "        cache = {}\n",
    "        \n",
    "        for image, label in input_data:\n",
    "\n",
    "            # Not normalizing image\n",
    "            image_emb = clip_model.encode_image(image.unsqueeze(0).cuda())\n",
    "\n",
    "            #speeeed up\n",
    "            if label in cache:\n",
    "                print(\"Using cache\")\n",
    "                class_embeddings = cache[label]\n",
    "            else:\n",
    "                # Normalizing Text\n",
    "                class_name = self.classes[label]\n",
    "                class_name = class_name.replace(\"_\", \" \")\n",
    "                text = clip.tokenize(self.phrase.format(class_name))\n",
    "                class_embeddings = clip_model.encode_text(text.cuda())\n",
    "                class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "                cache[label] = class_embeddings\n",
    "\n",
    "            final_input = torch.cat((image_emb,class_embeddings),dim=1)\n",
    "            new_input_data.append((final_input,label))\n",
    "\n",
    "\n",
    "\n",
    "        true_class_ids = list({x[1] for x in new_input_data})\n",
    "\n",
    "        all_images = torch.cat([x[0].unsqueeze(0) for x in new_input_data])\n",
    "        all_images = all_images.reshape(\n",
    "            (self.n_way, self.n_shot + self.n_query, *all_images.shape[1:])\n",
    "        )\n",
    "        # pylint: disable=not-callable\n",
    "        all_labels = torch.tensor(\n",
    "            [true_class_ids.index(x[1]) for x in new_input_data]\n",
    "        ).reshape((self.n_way, self.n_shot + self.n_query))\n",
    "        # pylint: enable=not-callable\n",
    "\n",
    "        support_images = all_images[:, : self.n_shot].reshape(\n",
    "            (-1, *all_images.shape[2:])\n",
    "        )\n",
    "        query_images = all_images[:, self.n_shot :].reshape((-1, *all_images.shape[2:]))\n",
    "        support_labels = all_labels[:, : self.n_shot].flatten()\n",
    "        query_labels = all_labels[:, self.n_shot :].flatten()\n",
    "        return (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            true_class_ids,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33ac596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device)\n",
    "phrase = \"This is a photo of a {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59992f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = Cifar100(5, 3)\n",
    "_,_, train_dataset, _ = dataset.get_train_loaders(transform_fn=clip_preprocess)\n",
    "train_dataset.labels = train_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2b99908",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_sampler = CLIPTaskSampler(train_dataset, n_way=3, n_shot=4, n_query=5, n_tasks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3440f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=task_sampler,\n",
    "    num_workers=0,\n",
    "    collate_fn=task_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d17f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[1, 4]\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n",
      "Using cache\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    example_support_images,\n",
    "    example_support_labels,\n",
    "    example_query_images,\n",
    "    example_query_labels,\n",
    "    example_class_ids,\n",
    ") = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f28d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc90d4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_query_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c068f27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97, 99, 44]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151af72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('p3': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd06250aeecf667241d2415373c88b70d4f824c8a2781323a01ee2a69a098796f4a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
