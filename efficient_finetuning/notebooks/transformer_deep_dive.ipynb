{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "20c2f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e808b75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e66bbf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from typing import Optional, Any, Union, Callable\n",
    "from torch import Tensor\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of the intermediate layer, can be a string\n",
    "            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n",
    "            operations, respectivaly. Otherwise it's done after. Default: ``False`` (after).\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None, kdim=None, vdim=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        if kdim == None:\n",
    "            kdim = d_model\n",
    "        if vdim == None:\n",
    "            vdim = d_model\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, kdim=kdim, vdim=vdim)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = _get_activation_fn(activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "\n",
    "        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n",
    "\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)\n",
    "            x = x + self._ff_block(self.norm2(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))\n",
    "            x = self.norm2(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x, attn = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=True)\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024, feedforward_dim=512, reduction_factor=2, kdim=None, vdim=None):\n",
    "        super().__init__()\n",
    "        d_model = int(embedding_dim/reduction_factor)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=1, dim_feedforward=feedforward_dim, kdim=kdim, vdim=vdim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.dim_reducer = nn.Linear(embedding_dim, d_model)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([], device=device))\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        output = self.dim_reducer(input)\n",
    "        output = output.permute(1,0,2)\n",
    "        output = self.transformer_encoder(output)\n",
    "        output = output.permute(1,0,2)\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d810ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer().to(device)\n",
    "arr = torch.rand((1,512,1024)).to(device)\n",
    "out = model(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "10552555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+------------+\n",
      "|                        Modules                         | Parameters |\n",
      "+--------------------------------------------------------+------------+\n",
      "|                      logit_scale                       |     1      |\n",
      "| transformer_encoder.layers.0.self_attn.in_proj_weight  |   786432   |\n",
      "|  transformer_encoder.layers.0.self_attn.in_proj_bias   |    1536    |\n",
      "| transformer_encoder.layers.0.self_attn.out_proj.weight |   262144   |\n",
      "|  transformer_encoder.layers.0.self_attn.out_proj.bias  |    512     |\n",
      "|      transformer_encoder.layers.0.linear1.weight       |   262144   |\n",
      "|       transformer_encoder.layers.0.linear1.bias        |    512     |\n",
      "|      transformer_encoder.layers.0.linear2.weight       |   262144   |\n",
      "|       transformer_encoder.layers.0.linear2.bias        |    512     |\n",
      "|       transformer_encoder.layers.0.norm1.weight        |    512     |\n",
      "|        transformer_encoder.layers.0.norm1.bias         |    512     |\n",
      "|       transformer_encoder.layers.0.norm2.weight        |    512     |\n",
      "|        transformer_encoder.layers.0.norm2.bias         |    512     |\n",
      "|                   dim_reducer.weight                   |   524288   |\n",
      "|                    dim_reducer.bias                    |    512     |\n",
      "+--------------------------------------------------------+------------+\n",
      "Total Trainable Params: 2102785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2102785"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5fb929ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1_hidden(nn.Module):\n",
    "    def __init__(self, input_size = 1024, hidden_size = 512):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.out = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([], device=device))\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.out(self.fc1(input))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cc80be85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|   Modules   | Parameters |\n",
      "+-------------+------------+\n",
      "| logit_scale |     1      |\n",
      "|  fc1.weight |   524288   |\n",
      "|   fc1.bias  |    512     |\n",
      "|  out.weight |   262144   |\n",
      "|   out.bias  |    512     |\n",
      "+-------------+------------+\n",
      "Total Trainable Params: 787457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "787457"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = MLP_1_hidden().to(device)\n",
    "count_parameters(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a8c976e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+------------+\n",
      "|                        Modules                         | Parameters |\n",
      "+--------------------------------------------------------+------------+\n",
      "|                      logit_scale                       |     1      |\n",
      "| transformer_encoder.layers.0.self_attn.in_proj_weight  |   196608   |\n",
      "|  transformer_encoder.layers.0.self_attn.in_proj_bias   |    768     |\n",
      "| transformer_encoder.layers.0.self_attn.out_proj.weight |   65536    |\n",
      "|  transformer_encoder.layers.0.self_attn.out_proj.bias  |    256     |\n",
      "|      transformer_encoder.layers.0.linear1.weight       |   131072   |\n",
      "|       transformer_encoder.layers.0.linear1.bias        |    512     |\n",
      "|      transformer_encoder.layers.0.linear2.weight       |   131072   |\n",
      "|       transformer_encoder.layers.0.linear2.bias        |    256     |\n",
      "|       transformer_encoder.layers.0.norm1.weight        |    256     |\n",
      "|        transformer_encoder.layers.0.norm1.bias         |    256     |\n",
      "|       transformer_encoder.layers.0.norm2.weight        |    256     |\n",
      "|        transformer_encoder.layers.0.norm2.bias         |    256     |\n",
      "|                   dim_reducer.weight                   |   262144   |\n",
      "|                    dim_reducer.bias                    |    256     |\n",
      "+--------------------------------------------------------+------------+\n",
      "Total Trainable Params: 789505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "789505"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = Transformer(1024, 512, 4).to(device)\n",
    "count_parameters(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "39434238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+------------+\n",
      "|                        Modules                         | Parameters |\n",
      "+--------------------------------------------------------+------------+\n",
      "|                      logit_scale                       |     1      |\n",
      "| transformer_encoder.layers.0.self_attn.in_proj_weight  |   786432   |\n",
      "|  transformer_encoder.layers.0.self_attn.in_proj_bias   |    1536    |\n",
      "| transformer_encoder.layers.0.self_attn.out_proj.weight |   262144   |\n",
      "|  transformer_encoder.layers.0.self_attn.out_proj.bias  |    512     |\n",
      "|      transformer_encoder.layers.0.linear1.weight       |   262144   |\n",
      "|       transformer_encoder.layers.0.linear1.bias        |    512     |\n",
      "|      transformer_encoder.layers.0.linear2.weight       |   262144   |\n",
      "|       transformer_encoder.layers.0.linear2.bias        |    512     |\n",
      "|       transformer_encoder.layers.0.norm1.weight        |    512     |\n",
      "|        transformer_encoder.layers.0.norm1.bias         |    512     |\n",
      "|       transformer_encoder.layers.0.norm2.weight        |    512     |\n",
      "|        transformer_encoder.layers.0.norm2.bias         |    512     |\n",
      "|                   dim_reducer.weight                   |   524288   |\n",
      "|                    dim_reducer.bias                    |    512     |\n",
      "+--------------------------------------------------------+------------+\n",
      "Total Trainable Params: 2102785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2102785"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = Transformer(1024, 512, 2).to(device)\n",
    "count_parameters(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1738081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+------------+\n",
      "|                        Modules                         | Parameters |\n",
      "+--------------------------------------------------------+------------+\n",
      "|                      logit_scale                       |     1      |\n",
      "|  transformer_encoder.layers.0.self_attn.q_proj_weight  |   262144   |\n",
      "|  transformer_encoder.layers.0.self_attn.k_proj_weight  |   28672    |\n",
      "|  transformer_encoder.layers.0.self_attn.v_proj_weight  |   28672    |\n",
      "|  transformer_encoder.layers.0.self_attn.in_proj_bias   |    1536    |\n",
      "| transformer_encoder.layers.0.self_attn.out_proj.weight |   262144   |\n",
      "|  transformer_encoder.layers.0.self_attn.out_proj.bias  |    512     |\n",
      "|      transformer_encoder.layers.0.linear1.weight       |   262144   |\n",
      "|       transformer_encoder.layers.0.linear1.bias        |    512     |\n",
      "|      transformer_encoder.layers.0.linear2.weight       |   262144   |\n",
      "|       transformer_encoder.layers.0.linear2.bias        |    512     |\n",
      "|       transformer_encoder.layers.0.norm1.weight        |    512     |\n",
      "|        transformer_encoder.layers.0.norm1.bias         |    512     |\n",
      "|       transformer_encoder.layers.0.norm2.weight        |    512     |\n",
      "|        transformer_encoder.layers.0.norm2.bias         |    512     |\n",
      "|                   dim_reducer.weight                   |   524288   |\n",
      "|                    dim_reducer.bias                    |    512     |\n",
      "+--------------------------------------------------------+------------+\n",
      "Total Trainable Params: 1635841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1635841"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = Transformer(embedding_dim=1024, feedforward_dim=512, reduction_factor=2, kdim=128, vdim=128).to(device)\n",
    "count_parameters(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bfe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
