{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d446b5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "import clip\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bd022ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=None, typed=False)\n",
    "def encode_image(img):\n",
    "    return clip_model.encode_image(img)\n",
    "\n",
    "class clip_encode_image(object):\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        img = img.unsqueeze(0)\n",
    "        img =  encode_image(img)\n",
    "        img = img.squeeze(0)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__+'()'\n",
    "\n",
    "class Cifar100Clip(Cifar100):\n",
    "    def __init__(self, num_workers, batch_size, root=None):\n",
    "\n",
    "        super().__init__(num_workers, batch_size, root)\n",
    "        \n",
    "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device)\n",
    "        self.name = \"CIFAR100Clip\"\n",
    "        \n",
    "        self.transform_fn = self.clip_preprocess\n",
    "        \n",
    "#         self.transform_fn.transforms.append(clip_encode_image())\n",
    "        \n",
    "        \n",
    "    def get_train_loaders(self, num_elements_per_class=-1):\n",
    "\n",
    "        transform_fn = self.transform_fn\n",
    "        \n",
    "        train_dataset = CIFAR100(\n",
    "            root=self.root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform_fn,\n",
    "        )\n",
    "#         d = train_dataset.data\n",
    "#         print(d.shape)\n",
    "#         d = torch.from_numpy(np.transpose( d, (0, 3, 1, 2)))\n",
    "#         print(d.shape)\n",
    "#         d = torch.stack([\n",
    "#     clip_model.encode_image(x_i.unsqueeze(0)) for i, x_i in enumerate(torch.unbind(d, dim=0), 0)\n",
    "# ], dim=axis)\n",
    "#         print(train_dataset.data)\n",
    "        \n",
    "#         train_dataset.data = d\n",
    "\n",
    "        if num_elements_per_class >=0:\n",
    "            train_dataset = get_n_items_per_class(train_dataset, num_elements_per_class)\n",
    "\n",
    "        num_train = len(train_dataset)\n",
    "        indices = list(range(num_train))\n",
    "        split = int(np.floor(self.valid_size * num_train))\n",
    "\n",
    "        train_idx = indices[split:]\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        \n",
    "        # Little hacky - need to improve\n",
    "        if self.classes == None:\n",
    "            self.classes = train_dataset.classes\n",
    "            \n",
    "        return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5ad4d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Cifar100Clip(6,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0ed34b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "loaders = c.get_train_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "113901fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i,l in loaders:\n",
    "    print(i.shape, l.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bb26fef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "dataset_obj = Cifar100Clip(6, 75)\n",
    "train_loader = dataset_obj.get_train_loaders()\n",
    "# test_loader = dataset_obj.get_test_loader()\n",
    "classes = dataset_obj.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8e5ec2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "    \n",
    "model = LogisticRegression(512,100)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 500\n",
    "\n",
    "def num_correct_preds(outputs, labels):\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "620940fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c652f8cd53424ed29f535f7af31cb29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/multimodal/model/multimodal_transformer/___torch_mangle_9591.py\", line 19, in encode_image\n    _0 = self.visual\n    input = torch.to(image, torch.device(\"cuda:0\"), 5, False, False, None)\n    return (_0).forward(input, )\n            ~~~~~~~~~~~ <--- HERE\n  def encode_text(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9591.Multimodal,\n    input: Tensor) -> Tensor:\n  File \"code/__torch__/multimodal/model/multimodal_transformer.py\", line 34, in forward\n    x2 = torch.add(x1, torch.to(_4, 5, False, False, None), alpha=1)\n    x3 = torch.permute((_3).forward(x2, ), [1, 0, 2])\n    x4 = torch.permute((_2).forward(x3, ), [1, 0, 2])\n                        ~~~~~~~~~~~ <--- HERE\n    _15 = torch.slice(x4, 0, 0, 9223372036854775807, 1)\n    x5 = torch.slice(torch.select(_15, 1, 0), 1, 0, 9223372036854775807, 1)\n  File \"code/__torch__/multimodal/model/multimodal_transformer/___torch_mangle_9477.py\", line 8, in forward\n  def forward(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9477.Transformer,\n    x: Tensor) -> Tensor:\n    return (self.resblocks).forward(x, )\n            ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n  def forward1(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9477.Transformer,\n    x: Tensor) -> Tensor:\n  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_9476.py\", line 32, in forward\n    _11 = (_7).forward((_8).forward((_9).forward(_10, ), ), )\n    _12 = (_4).forward((_5).forward((_6).forward(_11, ), ), )\n    _13 = (_1).forward((_2).forward((_3).forward(_12, ), ), )\n           ~~~~~~~~~~~ <--- HERE\n    return (_0).forward(_13, )\n  def forward1(self: __torch__.torch.nn.modules.container.___torch_mangle_9476.Sequential,\n  File \"code/__torch__/multimodal/model/multimodal_transformer/___torch_mangle_9466.py\", line 15, in forward\n    _2 = (self.attn).forward((self.ln_1).forward(argument_1, ), )\n    x = torch.add(argument_1, _2, alpha=1)\n    x0 = torch.add(x, (_0).forward((_1).forward(x, ), ), alpha=1)\n                       ~~~~~~~~~~~ <--- HERE\n    return x0\n  def forward1(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9466.ResidualAttentionBlock,\n  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_9464.py\", line 11, in forward\n    argument_1: Tensor) -> Tensor:\n    _0 = self.c_proj\n    _1 = (self.gelu).forward((self.c_fc).forward(argument_1, ), )\n          ~~~~~~~~~~~~~~~~~~ <--- HERE\n    return (_0).forward(_1, )\n  def forward1(self: __torch__.torch.nn.modules.container.___torch_mangle_9464.Sequential,\n  File \"code/__torch__/multimodal/model/multimodal_transformer/___torch_mangle_9462.py\", line 7, in forward\n  def forward(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9462.QuickGELU,\n    argument_1: Tensor) -> Tensor:\n    _0 = torch.sigmoid(torch.mul(argument_1, CONSTANTS.c2))\n         ~~~~~~~~~~~~~ <--- HERE\n    return torch.mul(argument_1, _0)\n  def forward1(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9462.QuickGELU,\n\nTraceback of TorchScript, original code (most recent call last):\n/root/workspace/multimodal-pytorch/multimodal/model/multimodal_transformer.py(26): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py(117): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/workspace/multimodal-pytorch/multimodal/model/multimodal_transformer.py(49): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py(117): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/workspace/multimodal-pytorch/multimodal/model/multimodal_transformer.py(63): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/workspace/multimodal-pytorch/multimodal/model/multimodal_transformer.py(93): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/workspace/multimodal-pytorch/multimodal/model/multimodal_transformer.py(221): visual_forward\n/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py(940): trace_module\n<ipython-input-1-40b054242c5d>(36): export_torchscript_models\n<ipython-input-2-808c11c4d1cf>(3): <module>\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3418): run_code\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3338): run_ast_nodes\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3147): run_cell_async\n/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py(68): _pseudo_sync_runner\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2923): _run_cell\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2878): run_cell\n/opt/conda/lib/python3.7/site-packages/IPython/terminal/interactiveshell.py(555): interact\n/opt/conda/lib/python3.7/site-packages/IPython/terminal/interactiveshell.py(564): mainloop\n/opt/conda/lib/python3.7/site-packages/IPython/terminal/ipapp.py(356): start\n/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py(845): launch_instance\n/opt/conda/lib/python3.7/site-packages/IPython/__init__.py(126): start_ipython\n/opt/conda/bin/ipython(8): <module>\nRuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 10.76 GiB total capacity; 3.00 GiB already allocated; 7.56 MiB free; 3.22 GiB reserved in total by PyTorch)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-35cc69b387f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/multimodal/model/multimodal_transformer/___torch_mangle_9591.py\", line 19, in encode_image\n    _0 = self.visual\n    input = torch.to(image, torch.device(\"cuda:0\"), 5, False, False, None)\n    return (_0).forward(input, )\n            ~~~~~~~~~~~ <--- HERE\n  def encode_text(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9591.Multimodal,\n    input: Tensor) -> Tensor:\n  File \"code/__torch__/multimodal/model/multimodal_transformer.py\", line 34, in forward\n    x2 = torch.add(x1, torch.to(_4, 5, False, False, None), alpha=1)\n    x3 = torch.permute((_3).forward(x2, ), [1, 0, 2])\n    x4 = torch.permute((_2).forward(x3, ), [1, 0, 2])\n                        ~~~~~~~~~~~ <--- HERE\n    _15 = torch.slice(x4, 0, 0, 9223372036854775807, 1)\n    x5 = torch.slice(torch.select(_15, 1, 0), 1, 0, 9223372036854775807, 1)\n  File \"code/__torch__/multimodal/model/multimodal_transformer/___torch_mangle_9477.py\", line 8, in forward\n  def forward(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9477.Transformer,\n    x: Tensor) -> Tensor:\n    return (self.resblocks).forward(x, )\n            ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n  def forward1(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9477.Transformer,\n    x: Tensor) -> Tensor:\n  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_9476.py\", line 32, in forward\n    _11 = (_7).forward((_8).forward((_9).forward(_10, ), ), )\n    _12 = (_4).forward((_5).forward((_6).forward(_11, ), ), )\n    _13 = (_1).forward((_2).forward((_3).forward(_12, ), ), )\n           ~~~~~~~~~~~ <--- HERE\n    return (_0).forward(_13, )\n  def forward1(self: __torch__.torch.nn.modules.container.___torch_mangle_9476.Sequential,\n  File \"code/__torch__/multimodal/model/multimodal_transformer/___torch_mangle_9466.py\", line 15, in forward\n    _2 = (self.attn).forward((self.ln_1).forward(argument_1, ), )\n    x = torch.add(argument_1, _2, alpha=1)\n    x0 = torch.add(x, (_0).forward((_1).forward(x, ), ), alpha=1)\n                       ~~~~~~~~~~~ <--- HERE\n    return x0\n  def forward1(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9466.ResidualAttentionBlock,\n  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_9464.py\", line 11, in forward\n    argument_1: Tensor) -> Tensor:\n    _0 = self.c_proj\n    _1 = (self.gelu).forward((self.c_fc).forward(argument_1, ), )\n          ~~~~~~~~~~~~~~~~~~ <--- HERE\n    return (_0).forward(_1, )\n  def forward1(self: __torch__.torch.nn.modules.container.___torch_mangle_9464.Sequential,\n  File \"code/__torch__/multimodal/model/multimodal_transformer/___torch_mangle_9462.py\", line 7, in forward\n  def forward(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9462.QuickGELU,\n    argument_1: Tensor) -> Tensor:\n    _0 = torch.sigmoid(torch.mul(argument_1, CONSTANTS.c2))\n         ~~~~~~~~~~~~~ <--- HERE\n    return torch.mul(argument_1, _0)\n  def forward1(self: __torch__.multimodal.model.multimodal_transformer.___torch_mangle_9462.QuickGELU,\n\nTraceback of TorchScript, original code (most recent call last):\n/root/workspace/multimodal-pytorch/multimodal/model/multimodal_transformer.py(26): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py(117): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/workspace/multimodal-pytorch/multimodal/model/multimodal_transformer.py(49): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py(117): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/workspace/multimodal-pytorch/multimodal/model/multimodal_transformer.py(63): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/workspace/multimodal-pytorch/multimodal/model/multimodal_transformer.py(93): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/workspace/multimodal-pytorch/multimodal/model/multimodal_transformer.py(221): visual_forward\n/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py(940): trace_module\n<ipython-input-1-40b054242c5d>(36): export_torchscript_models\n<ipython-input-2-808c11c4d1cf>(3): <module>\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3418): run_code\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3338): run_ast_nodes\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3147): run_cell_async\n/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py(68): _pseudo_sync_runner\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2923): _run_cell\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2878): run_cell\n/opt/conda/lib/python3.7/site-packages/IPython/terminal/interactiveshell.py(555): interact\n/opt/conda/lib/python3.7/site-packages/IPython/terminal/interactiveshell.py(564): mainloop\n/opt/conda/lib/python3.7/site-packages/IPython/terminal/ipapp.py(356): start\n/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py(845): launch_instance\n/opt/conda/lib/python3.7/site-packages/IPython/__init__.py(126): start_ipython\n/opt/conda/bin/ipython(8): <module>\nRuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 10.76 GiB total capacity; 3.00 GiB already allocated; 7.56 MiB free; 3.22 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "model = model.to(torch.float16)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        \n",
    "        inputs = clip_model.encode_image(inputs.to(device))\n",
    "        \n",
    "        inputs = inputs / inputs.norm(dim=-1, keepdim=True)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss+=loss\n",
    "        total += len(labels)\n",
    "        correct += num_correct_preds(outputs, labels)\n",
    "        \n",
    "        \n",
    "    epoch_loss = running_loss/len(train_loader)\n",
    "    epoch_accuracy = correct*100/total\n",
    "    print(\n",
    "        f\"Training: Epoch {epoch} || Loss: {epoch_loss:7.3f} || Accuracy: {epoch_accuracy:6.2f}%\"\n",
    "    )\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367f378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
