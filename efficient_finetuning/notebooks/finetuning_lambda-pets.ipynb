{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17b7e0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "phrase = \"This is a photo of a {}, an animal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bca3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_image_features(data_loader):\n",
    "    \"\"\"Given a dataloader object, generate two torch arrays of encoded images and corresponding labels\"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    global clip_model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader):\n",
    "            features = clip_model.encode_image(images.to(device))\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features), torch.cat(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f75258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings from the linear probe\n",
    "def get_clip_linear_probe_classifier(\n",
    "    train_features, train_labels, C=1\n",
    "):\n",
    "\n",
    "    classifier = LogisticRegression(C=C, max_iter=1000, n_jobs=6)\n",
    "    classifier.fit(train_features.cpu().numpy(), train_labels.cpu().numpy())\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def get_clip_linear_probe_embedding(classifier, imgs):\n",
    "    return torch.from_numpy(classifier.predict_proba(imgs.cpu().detach().numpy())).to(torch.float16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb08fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for \n",
    "\n",
    "def get_clip_text_features(classes):\n",
    "    \"\"\"Given a dataloader object, generate two torch arrays of encoded images and corresponding labels\"\"\"\n",
    "    # Assumes the positions are in accordance to the label numbers\n",
    "    embedding_per_class = {}\n",
    "    \n",
    "    global clip_model\n",
    "\n",
    "    global phrase\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i,_class in enumerate(classes):\n",
    "            _class = _class.replace(\"_\", \" \")\n",
    "            text = clip.tokenize(phrase.format(_class)).cuda() \n",
    "            class_embeddings = clip_model.encode_text(\n",
    "                    text\n",
    "                )\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            embedding_per_class[i] = class_embeddings\n",
    "    return embedding_per_class\n",
    "\n",
    "def get_text_embeds(classes):\n",
    "    text_clip_features = get_clip_text_features(classes)\n",
    "    text_embeds = []\n",
    "    for c in range(len(classes)):\n",
    "        text_embs = text_clip_features[c].squeeze()\n",
    "        text_embeds.append(text_embs)\n",
    "\n",
    "    text_embeds = torch.stack(text_embeds).squeeze(1)\n",
    "    return text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de544469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FinetuneLambda(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.tensor([0.0]).cuda())\n",
    "        self.b = torch.nn.Parameter(torch.tensor([0.0]).cuda())\n",
    "        self.sftmx = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, image_image, image_text):\n",
    "        out = self.a*(image_image) + (self.b)*self.sftmx((image_text)*100)\n",
    "        return out[0]\n",
    "    \n",
    "    def string(self):\n",
    "        return f'A: {self.a.item()}, B: {self.b.item()}'\n",
    "    \n",
    "model = FinetuneLambda()\n",
    "model.logit_scale = nn.Parameter(torch.ones([], device=device))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "num_epochs = 500\n",
    "\n",
    "def num_correct_preds(outputs, labels):\n",
    "    predicted = outputs.argmax().item()\n",
    "    labels = labels.item()\n",
    "    return predicted == labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "181166aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771366f6aec14e9c85cea46cddeaea4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import *\n",
    "dataset_obj = OxfordPets(4, 1)\n",
    "n_classes = 32\n",
    "\n",
    "train_loader, _ = dataset_obj.get_train_loaders(transform_fn=clip_preprocess,num_elements_per_class=n_classes)\n",
    "test_loader = dataset_obj.get_test_loader(transform_fn=clip_preprocess)\n",
    "train_features, train_labels = get_clip_image_features(train_loader)\n",
    "classes = dataset_obj.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2d99fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc = get_clip_linear_probe_classifier(train_features, train_labels)\n",
    "text_embeddings_per_class = get_text_embeds(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bca375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2563b1f8d945aabfebc82d0508b2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6567782f13854389936b2127fb5fc19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  84.27364404469883 %\n",
      "----------------\n",
      "Training: Epoch 0 || Loss:   0.001 || A: 10.92768669128418, B: 3.8314263820648193\n",
      "Training: Epoch 1 || Loss:   0.001 || A: 10.930469512939453, B: 3.829740047454834\n",
      "Training: Epoch 2 || Loss:   0.001 || A: 10.933243751525879, B: 3.828058958053589\n",
      "Training: Epoch 3 || Loss:   0.001 || A: 10.936013221740723, B: 3.8263823986053467\n",
      "Training: Epoch 4 || Loss:   0.001 || A: 10.938776016235352, B: 3.8247110843658447\n",
      "Training: Epoch 5 || Loss:   0.001 || A: 10.941534996032715, B: 3.823042869567871\n",
      "Training: Epoch 6 || Loss:   0.001 || A: 10.944287300109863, B: 3.8213789463043213\n",
      "Training: Epoch 7 || Loss:   0.001 || A: 10.947032928466797, B: 3.8197193145751953\n",
      "Training: Epoch 8 || Loss:   0.001 || A: 10.949771881103516, B: 3.818063974380493\n",
      "Training: Epoch 9 || Loss:   0.001 || A: 10.952506065368652, B: 3.816413164138794\n",
      "Training: Epoch 10 || Loss:   0.001 || A: 10.955232620239258, B: 3.8147671222686768\n",
      "Training: Epoch 11 || Loss:   0.001 || A: 10.9579496383667, B: 3.8131256103515625\n",
      "Training: Epoch 12 || Loss:   0.001 || A: 10.960657119750977, B: 3.811488151550293\n",
      "Training: Epoch 13 || Loss:   0.001 || A: 10.963358879089355, B: 3.809854745864868\n",
      "Training: Epoch 14 || Loss:   0.001 || A: 10.966055870056152, B: 3.8082263469696045\n",
      "Training: Epoch 15 || Loss:   0.001 || A: 10.96874713897705, B: 3.8066020011901855\n",
      "Training: Epoch 16 || Loss:   0.001 || A: 10.971435546875, B: 3.8049817085266113\n",
      "Training: Epoch 17 || Loss:   0.001 || A: 10.974120140075684, B: 3.803365468978882\n",
      "Training: Epoch 18 || Loss:   0.001 || A: 10.976799011230469, B: 3.801753282546997\n",
      "Training: Epoch 19 || Loss:   0.001 || A: 10.979472160339355, B: 3.800144672393799\n",
      "Training: Epoch 20 || Loss:   0.001 || A: 10.982136726379395, B: 3.7985403537750244\n",
      "Training: Epoch 21 || Loss:   0.001 || A: 10.9847993850708, B: 3.7969400882720947\n",
      "Training: Epoch 22 || Loss:   0.001 || A: 10.987452507019043, B: 3.795344114303589\n",
      "Training: Epoch 23 || Loss:   0.001 || A: 10.990099906921387, B: 3.793752908706665\n",
      "Training: Epoch 24 || Loss:   0.001 || A: 10.9927396774292, B: 3.792165517807007\n",
      "Training: Epoch 25 || Loss:   0.001 || A: 10.995373725891113, B: 3.7905824184417725\n",
      "Training: Epoch 26 || Loss:   0.001 || A: 10.998002052307129, B: 3.789003610610962\n",
      "Training: Epoch 27 || Loss:   0.001 || A: 11.000624656677246, B: 3.7874293327331543\n",
      "Training: Epoch 28 || Loss:   0.001 || A: 11.003241539001465, B: 3.7858588695526123\n",
      "Training: Epoch 29 || Loss:   0.001 || A: 11.005851745605469, B: 3.7842931747436523\n",
      "Training: Epoch 30 || Loss:   0.001 || A: 11.008453369140625, B: 3.7827305793762207\n",
      "Training: Epoch 31 || Loss:   0.001 || A: 11.011052131652832, B: 3.781172752380371\n",
      "Training: Epoch 32 || Loss:   0.001 || A: 11.013641357421875, B: 3.7796192169189453\n",
      "Training: Epoch 33 || Loss:   0.001 || A: 11.01622486114502, B: 3.7780685424804688\n",
      "Training: Epoch 34 || Loss:   0.001 || A: 11.018802642822266, B: 3.7765228748321533\n",
      "Training: Epoch 35 || Loss:   0.001 || A: 11.021376609802246, B: 3.7749810218811035\n",
      "Training: Epoch 36 || Loss:   0.001 || A: 11.023942947387695, B: 3.7734429836273193\n",
      "Training: Epoch 37 || Loss:   0.001 || A: 11.026504516601562, B: 3.7719080448150635\n",
      "Training: Epoch 38 || Loss:   0.001 || A: 11.029060363769531, B: 3.7703769207000732\n",
      "Training: Epoch 39 || Loss:   0.001 || A: 11.031608581542969, B: 3.7688496112823486\n",
      "Training: Epoch 40 || Loss:   0.001 || A: 11.034150123596191, B: 3.7673258781433105\n",
      "Training: Epoch 41 || Loss:   0.001 || A: 11.036688804626465, B: 3.765806198120117\n",
      "Training: Epoch 42 || Loss:   0.001 || A: 11.039220809936523, B: 3.7642908096313477\n",
      "Training: Epoch 43 || Loss:   0.001 || A: 11.041749000549316, B: 3.7627789974212646\n",
      "Training: Epoch 44 || Loss:   0.001 || A: 11.044269561767578, B: 3.761270523071289\n",
      "Training: Epoch 45 || Loss:   0.001 || A: 11.046781539916992, B: 3.7597649097442627\n",
      "Training: Epoch 46 || Loss:   0.001 || A: 11.049283981323242, B: 3.75826358795166\n",
      "Training: Epoch 47 || Loss:   0.001 || A: 11.051780700683594, B: 3.756765842437744\n",
      "Training: Epoch 48 || Loss:   0.001 || A: 11.054271697998047, B: 3.7552719116210938\n",
      "Training: Epoch 49 || Loss:   0.001 || A: 11.056755065917969, B: 3.7537810802459717\n",
      "Training: Epoch 50 || Loss:   0.001 || A: 11.059231758117676, B: 3.7522945404052734\n",
      "Training: Epoch 51 || Loss:   0.001 || A: 11.061698913574219, B: 3.7508113384246826\n",
      "Training: Epoch 52 || Loss:   0.001 || A: 11.064160346984863, B: 3.74933123588562\n",
      "Training: Epoch 53 || Loss:   0.001 || A: 11.066614151000977, B: 3.747854232788086\n",
      "Training: Epoch 54 || Loss:   0.001 || A: 11.06905746459961, B: 3.74638032913208\n",
      "Training: Epoch 55 || Loss:   0.001 || A: 11.071491241455078, B: 3.7449097633361816\n",
      "Training: Epoch 56 || Loss:   0.001 || A: 11.073918342590332, B: 3.743443489074707\n",
      "Training: Epoch 57 || Loss:   0.001 || A: 11.076340675354004, B: 3.741981029510498\n",
      "Training: Epoch 58 || Loss:   0.001 || A: 11.078758239746094, B: 3.7405223846435547\n",
      "Training: Epoch 59 || Loss:   0.001 || A: 11.081170082092285, B: 3.7390670776367188\n",
      "Training: Epoch 60 || Loss:   0.001 || A: 11.083577156066895, B: 3.737614870071411\n",
      "Training: Epoch 61 || Loss:   0.001 || A: 11.085976600646973, B: 3.736166000366211\n",
      "Training: Epoch 62 || Loss:   0.001 || A: 11.08836841583252, B: 3.734720468521118\n",
      "Training: Epoch 63 || Loss:   0.001 || A: 11.090751647949219, B: 3.7332780361175537\n",
      "Training: Epoch 64 || Loss:   0.001 || A: 11.09312629699707, B: 3.7318382263183594\n",
      "Training: Epoch 65 || Loss:   0.001 || A: 11.09549331665039, B: 3.7304039001464844\n",
      "Training: Epoch 66 || Loss:   0.001 || A: 11.097851753234863, B: 3.7289726734161377\n",
      "Training: Epoch 67 || Loss:   0.001 || A: 11.10020637512207, B: 3.7275443077087402\n",
      "Training: Epoch 68 || Loss:   0.001 || A: 11.102556228637695, B: 3.72611927986145\n",
      "Training: Epoch 69 || Loss:   0.001 || A: 11.104900360107422, B: 3.7246975898742676\n",
      "Training: Epoch 70 || Loss:   0.001 || A: 11.107236862182617, B: 3.7232794761657715\n",
      "Training: Epoch 71 || Loss:   0.001 || A: 11.109567642211914, B: 3.7218644618988037\n",
      "Training: Epoch 72 || Loss:   0.001 || A: 11.111895561218262, B: 3.720452308654785\n",
      "Training: Epoch 73 || Loss:   0.001 || A: 11.114218711853027, B: 3.7190442085266113\n",
      "Training: Epoch 74 || Loss:   0.001 || A: 11.116533279418945, B: 3.717639684677124\n",
      "Training: Epoch 75 || Loss:   0.001 || A: 11.118844985961914, B: 3.7162387371063232\n",
      "Training: Epoch 76 || Loss:   0.001 || A: 11.121148109436035, B: 3.7148404121398926\n",
      "Training: Epoch 77 || Loss:   0.001 || A: 11.123445510864258, B: 3.7134451866149902\n",
      "Training: Epoch 78 || Loss:   0.001 || A: 11.125738143920898, B: 3.712053060531616\n",
      "Training: Epoch 79 || Loss:   0.001 || A: 11.128023147583008, B: 3.7106640338897705\n",
      "Training: Epoch 80 || Loss:   0.001 || A: 11.130302429199219, B: 3.7092783451080322\n",
      "Training: Epoch 81 || Loss:   0.001 || A: 11.132577896118164, B: 3.707895040512085\n",
      "Training: Epoch 82 || Loss:   0.001 || A: 11.134846687316895, B: 3.7065160274505615\n",
      "Training: Epoch 83 || Loss:   0.001 || A: 11.13710880279541, B: 3.7051403522491455\n",
      "Training: Epoch 84 || Loss:   0.001 || A: 11.139362335205078, B: 3.703768014907837\n",
      "Training: Epoch 85 || Loss:   0.001 || A: 11.14161205291748, B: 3.7023978233337402\n",
      "Training: Epoch 86 || Loss:   0.001 || A: 11.143854141235352, B: 3.701031446456909\n",
      "Training: Epoch 87 || Loss:   0.001 || A: 11.146089553833008, B: 3.699667453765869\n",
      "Training: Epoch 88 || Loss:   0.001 || A: 11.148321151733398, B: 3.6983065605163574\n",
      "Training: Epoch 89 || Loss:   0.001 || A: 11.150545120239258, B: 3.696948289871216\n",
      "Training: Epoch 90 || Loss:   0.001 || A: 11.152764320373535, B: 3.6955924034118652\n",
      "Training: Epoch 91 || Loss:   0.001 || A: 11.154980659484863, B: 3.6942391395568848\n",
      "Training: Epoch 92 || Loss:   0.001 || A: 11.15719223022461, B: 3.6928884983062744\n",
      "Training: Epoch 93 || Loss:   0.001 || A: 11.159398078918457, B: 3.6915409564971924\n",
      "Training: Epoch 94 || Loss:   0.001 || A: 11.161591529846191, B: 3.6901967525482178\n",
      "Training: Epoch 95 || Loss:   0.001 || A: 11.163774490356445, B: 3.688854932785034\n",
      "Training: Epoch 96 || Loss:   0.001 || A: 11.165952682495117, B: 3.687516689300537\n",
      "Training: Epoch 97 || Loss:   0.001 || A: 11.168126106262207, B: 3.6861813068389893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 98 || Loss:   0.001 || A: 11.170295715332031, B: 3.6848483085632324\n",
      "Training: Epoch 99 || Loss:   0.001 || A: 11.172457695007324, B: 3.6835174560546875\n",
      "Training: Epoch 100 || Loss:   0.001 || A: 11.174613952636719, B: 3.6821887493133545\n",
      "Training: Epoch 101 || Loss:   0.001 || A: 11.176765441894531, B: 3.68086314201355\n",
      "Training: Epoch 102 || Loss:   0.001 || A: 11.178913116455078, B: 3.6795411109924316\n",
      "Training: Epoch 103 || Loss:   0.001 || A: 11.181056022644043, B: 3.6782217025756836\n",
      "Training: Epoch 104 || Loss:   0.001 || A: 11.183196067810059, B: 3.6769051551818848\n",
      "Training: Epoch 105 || Loss:   0.001 || A: 11.185327529907227, B: 3.6755917072296143\n",
      "Training: Epoch 106 || Loss:   0.001 || A: 11.187454223632812, B: 3.674281358718872\n",
      "Training: Epoch 107 || Loss:   0.001 || A: 11.1895751953125, B: 3.672973394393921\n",
      "Training: Epoch 108 || Loss:   0.001 || A: 11.191691398620605, B: 3.671668767929077\n",
      "Training: Epoch 109 || Loss:   0.001 || A: 11.193801879882812, B: 3.670366048812866\n",
      "Training: Epoch 110 || Loss:   0.001 || A: 11.195907592773438, B: 3.6690664291381836\n",
      "Training: Epoch 111 || Loss:   0.001 || A: 11.198010444641113, B: 3.66776967048645\n",
      "Training: Epoch 112 || Loss:   0.001 || A: 11.20010757446289, B: 3.6664750576019287\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for epoch in tqdm(range(num_epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    \n",
    "\n",
    "    for inputs, labels in zip(train_features, train_labels):\n",
    "        \n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        image_only = get_clip_linear_probe_embedding(lpc, inputs.repeat(len(classes), 1))\n",
    "        \n",
    "        inputs_norm = inputs/inputs.norm(dim=-1, keepdim=True)\n",
    "        image_text =inputs_norm.repeat(len(classes), 1) @ text_embeddings_per_class.T\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(image_only, image_text)\n",
    "        \n",
    "        loss = criterion(outputs.unsqueeze(0), labels.unsqueeze(0))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss+=loss\n",
    "        total += 1\n",
    "        correct += num_correct_preds(outputs, labels)\n",
    "        \n",
    "    if epoch%250 == 0:\n",
    "\n",
    "        model.eval()\n",
    "        eval_total = 0\n",
    "        eval_correct = 0\n",
    "        for images, target in tqdm(test_loader):\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "            image_features = clip_model.encode_image(images)\n",
    "            image_features_norm = image_features/image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            image_only = get_clip_linear_probe_embedding(lpc, image_features.repeat(len(classes), 1))\n",
    "            image_text = inputs_norm.repeat(len(classes), 1) @ text_embeddings_per_class.T\n",
    "\n",
    "            outputs = model(image_only, image_text)\n",
    "\n",
    "            eval_correct += num_correct_preds(outputs, target)\n",
    "            eval_total+=1\n",
    "        print(\"----------------\")\n",
    "        print(\"Accuracy: \", (eval_correct)*100/eval_total, \"%\" )\n",
    "        print(\"----------------\")\n",
    "        \n",
    "        \n",
    "    epoch_loss = running_loss/len(train_loader)\n",
    "    epoch_accuracy = correct*100/total\n",
    "    print(\n",
    "        f\"Training: Epoch {epoch} || Loss: {epoch_loss:7.3f} || {model.string()}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f25c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('p3': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd06250aeecf667241d2415373c88b70d4f824c8a2781323a01ee2a69a098796f4a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
