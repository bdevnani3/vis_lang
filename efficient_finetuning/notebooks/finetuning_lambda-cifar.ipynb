{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e472c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "0.8.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b7e0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "phrase = \"This is a photo of a {}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bca3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_image_features(data_loader):\n",
    "    \"\"\"Given a dataloader object, generate two torch arrays of encoded images and corresponding labels\"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    global clip_model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader):\n",
    "            features = clip_model.encode_image(images.to(device))\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features), torch.cat(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f75258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings from the linear probe\n",
    "def get_clip_linear_probe_classifier(\n",
    "    train_features, train_labels, C=1\n",
    "):\n",
    "\n",
    "    classifier = LogisticRegression(C=C, max_iter=1000, n_jobs=6)\n",
    "    classifier.fit(train_features.cpu().numpy(), train_labels.cpu().numpy())\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def get_clip_linear_probe_embedding(classifier, imgs):\n",
    "    return torch.from_numpy(classifier.predict_proba(imgs.cpu().detach().numpy())).to(torch.float16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb08fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for \n",
    "\n",
    "def get_clip_text_features(classes):\n",
    "    \"\"\"Given a dataloader object, generate two torch arrays of encoded images and corresponding labels\"\"\"\n",
    "    # Assumes the positions are in accordance to the label numbers\n",
    "    embedding_per_class = {}\n",
    "    \n",
    "    global clip_model\n",
    "\n",
    "    global phrase\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i,_class in enumerate(classes):\n",
    "            _class = _class.replace(\"_\", \" \")\n",
    "            text = clip.tokenize(phrase.format(_class)).cuda() \n",
    "            class_embeddings = clip_model.encode_text(\n",
    "                    text\n",
    "                )\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            embedding_per_class[i] = class_embeddings\n",
    "    return embedding_per_class\n",
    "\n",
    "def get_text_embeds(classes):\n",
    "    text_clip_features = get_clip_text_features(classes)\n",
    "    text_embeds = []\n",
    "    for c in range(len(classes)):\n",
    "        text_embs = text_clip_features[c].squeeze()\n",
    "        text_embeds.append(text_embs)\n",
    "\n",
    "    text_embeds = torch.stack(text_embeds).squeeze(1)\n",
    "    return text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de544469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FinetuneLambda(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.tensor([1.0]).cuda())\n",
    "        self.b = torch.nn.Parameter(torch.tensor([0.0]).cuda())\n",
    "        self.sftmx = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, image_image, image_text):\n",
    "        out = (self.a*(image_image) + (self.b)*self.sftmx((image_text)*100))/(self.a+ self.b)\n",
    "        return out[0]\n",
    "    \n",
    "    def string(self):\n",
    "        return f'A: {self.a.item()}, B: {self.b.item()}'\n",
    "    \n",
    "model = FinetuneLambda()\n",
    "model.logit_scale = nn.Parameter(torch.ones([], device=device))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "num_epochs = 500\n",
    "\n",
    "def num_correct_preds(outputs, labels):\n",
    "    predicted = outputs.argmax().item()\n",
    "    labels = labels.item()\n",
    "    return predicted == labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "181166aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n",
      "HERE\n",
      "3200\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706b9decb8e744f588ea58380463631e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from .. import datasets\n",
    "from datasets import *\n",
    "dataset_obj = Cifar100(4, 1)\n",
    "n_classes = 32\n",
    "\n",
    "train_loader, _ = dataset_obj.get_train_loaders(transform_fn=clip_preprocess,num_elements_per_class=n_classes)\n",
    "test_loader = dataset_obj.get_test_loader(transform_fn=clip_preprocess)\n",
    "train_features, train_labels = get_clip_image_features(train_loader)\n",
    "classes = dataset_obj.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2d99fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc = get_clip_linear_probe_classifier(train_features, train_labels)\n",
    "text_embeddings_per_class = get_text_embeds(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bca375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b959a86293f4895a200edcdfaa70073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80566c5b45cc4894af10bb05832d171e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Accuracy:  71.1 %\n",
      "----------------\n",
      "Training: Epoch 0 || Loss:   0.010 || A: 14.014772415161133, B: 4.008363246917725\n",
      "Training: Epoch 1 || Loss:   0.009 || A: 14.064529418945312, B: 3.993220329284668\n",
      "Training: Epoch 2 || Loss:   0.009 || A: 14.11314868927002, B: 3.9781816005706787\n",
      "Training: Epoch 3 || Loss:   0.009 || A: 14.160688400268555, B: 3.9633941650390625\n",
      "Training: Epoch 4 || Loss:   0.009 || A: 14.207194328308105, B: 3.948850631713867\n",
      "Training: Epoch 5 || Loss:   0.009 || A: 14.252708435058594, B: 3.934544324874878\n",
      "Training: Epoch 6 || Loss:   0.009 || A: 14.297274589538574, B: 3.920464277267456\n",
      "Training: Epoch 7 || Loss:   0.008 || A: 14.340943336486816, B: 3.9066009521484375\n",
      "Training: Epoch 8 || Loss:   0.008 || A: 14.383710861206055, B: 3.892948627471924\n",
      "Training: Epoch 9 || Loss:   0.008 || A: 14.42564868927002, B: 3.8795013427734375\n",
      "Training: Epoch 10 || Loss:   0.008 || A: 14.466766357421875, B: 3.8662497997283936\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for epoch in tqdm(range(num_epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    \n",
    "\n",
    "    for inputs, labels in zip(train_features, train_labels):\n",
    "        \n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        image_only = get_clip_linear_probe_embedding(lpc, inputs.repeat(len(classes), 1))\n",
    "        \n",
    "        inputs_norm = inputs/inputs.norm(dim=-1, keepdim=True)\n",
    "        image_text =inputs_norm.repeat(len(classes), 1) @ text_embeddings_per_class.T\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(image_only, image_text)\n",
    "        \n",
    "        loss = criterion(outputs.unsqueeze(0), labels.unsqueeze(0))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss+=loss\n",
    "        total += 1\n",
    "        correct += num_correct_preds(outputs, labels)\n",
    "        \n",
    "    if epoch%250 == 0:\n",
    "\n",
    "        model.eval()\n",
    "        eval_total = 0\n",
    "        eval_correct = 0\n",
    "        for images, target in tqdm(test_loader):\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "            image_features = clip_model.encode_image(images)\n",
    "            image_features_norm = image_features/image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            image_only = get_clip_linear_probe_embedding(lpc, image_features.repeat(len(classes), 1))\n",
    "            image_text = inputs_norm.repeat(len(classes), 1) @ text_embeddings_per_class.T\n",
    "\n",
    "            outputs = model(image_only, image_text)\n",
    "\n",
    "            eval_correct += num_correct_preds(outputs, target)\n",
    "            eval_total+=1\n",
    "        print(\"----------------\")\n",
    "        print(\"Accuracy: \", (eval_correct)*100/eval_total, \"%\" )\n",
    "        print(\"----------------\")\n",
    "        \n",
    "        \n",
    "    epoch_loss = running_loss/len(train_loader)\n",
    "    epoch_accuracy = correct*100/total\n",
    "    print(\n",
    "        f\"Training: Epoch {epoch} || Loss: {epoch_loss:7.3f} || {model.string()}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f25c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
