#### Dataset Configs
train_datasets: "cifar100"


#### Training Configs

# n_ways: [10, 5]
# n_ways: [102, 51, 25]
n_ways: [100, 50, 5]
n_shots: [16, 8, 4, 2, 1]
n_episodes: [5]
n_epochs: 1000
batch_size: 5


#### Model Configs
model: "transformer_encoder"
# model: "mlp" 

# Lamda
lambda:
  lr: 0.0001

# MLP
mlp:
  lr: 0.0001


# Transformer
transformer_encoder:
  lr: 0.0005
  feedforward_dim: 512
  reduction_factor: 2
  kdim: -1
  vdim: -1
  n_layers: 2
  loss_reduction: "avg"
  n_heads: 1
