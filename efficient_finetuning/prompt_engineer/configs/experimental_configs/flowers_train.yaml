#### Dataset Configs
train_datasets: "flowers"


#### Training Configs

# n_ways: [102, 51, 25, 5]
n_ways: [102]
n_shots: [1]
n_episodes: [5]
n_epochs: 500
batch_size: 5


#### Model Configs
# model: "transformer_encoder"
model: "mlp" 

# Lamda
lambda:
  lr: 0.0001

# MLP
mlp:
  lr: 0.0001


# Transformer
transformer_encoder:
  lr: 0.0005
  feedforward_dim: 512
  reduction_factor: 2
  kdim: -1
  vdim: -1
  n_layers: 2
  loss_reduction: "avg"
  n_heads: 1
